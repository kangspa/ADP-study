# MapReduce

- 구글에서 **분산 병렬 컴퓨팅**을 이용해 대용량 데이터 처리를 위해 제작한 소프트웨어 프레임워크 (2004년 "MapReduce:Simplified Data Processing on Large Clusters" 논문으로 공개)
- 분할정복 방식으로 **대용량 데이터를 병렬로 처리**할 수 있는 프로그래밍 모델
    - 분할정복 : 해결하려는 문제를 성질이 같은 여러 부분으로 나누어 해결 후, 원래 문제의 해를 구하는 방식
- C++, Java 등의 언어로 적용 가능하며, 아파치 하둡에서 오픈 소스 프로젝트로 시작한 Java 기반의 "Hadoop MapReduce" 프레임워크가 동일한 기능을 지원
- Client의 수행 작업 단위는 맵리듀스 잡(MapReduce Job)이라고 하며, Job은 Map Task와 Reduce Task로 나뉘어 실행
- 특별한 옵션을 주지 않으면, **Map Task 하나가 1개의 블록(64MB) 대상으로 연산 수행**
    - 해당 과정에서 생성된 중간 결과물들을 **사용자가 지정한 개수에 해당되는 Reduce Task들**이 받아와서 정렬 및 필터링 작업 후 최종 결과물 생성

## 구글 MapReduce

- 대용량 데이터 처리에 발생 가능한 복잡성을 추상화시켜, 핵심 기능 구현에만 집중할 수 있도록 개발

### 프로그래밍 모델 : Map과 Reduce 2개 단계

- **Map에서 Key와 Value의 쌍들을 입력**받는다.
    - 사용자가 정의한 **Map 함수를 거치며 다수의 새로운 Key, Value 쌍들로 변환** 후 로컬 파일 시스템에 임시 저장된다.
- 임시 파일들은 프레임워크에 의해 Reduce에게 전송된다.
    - 해당 과정에서 자동으로 **Shuffling과 Group by 정렬**의 과정을 거친다.
    - **Key와 Value의 리스트 형태**로 **Reduce의 입력** 레코드로 들어간다.
    - 사용자 정의 Reduce 함수를 통해 최종 Output으로 산출된다.
- 사용자 관점에서 세세한 이슈 신경 쓸 필요없이, Map과 Reduce 함수만으로 대규모 병렬 연산 작업 수행이 가능해진다.

### 실행 과정

1. MapReduce 프로그램 작성해서 실행한다.
2. 마스터는 사용자 프로그램에서 지정한 입력 데이터소스를 가지고 스케줄링한다.
3. 하나의 큰 파일을 여러 개의 파일 Split들로 나눈다.
    - **각 Split들이 Map 프로세스들의 할당 단위**
    - **Split 단위는 블록 사이즈인 64MB or 128MB**
4. **Split 수만큼 Map Task들이 워커로부터 Fork되고**, 동시에 실행돼 출력을 로컬 파일시스템에 저장한다.
5. Output 값들은 **Partitioner**라는 Reduce 번호를 할당해 주는 클래스를 통해 어떤 Reduce에게 보내질지 정해진다.
    - 특별히 정해지지 않으면, Key와 해시 값을 Reduce의 개수로 Modular 계산한 값이 부여되어 **동일한 Key들은 같은 Reduce로 배정**된다.
6. Map 단계가 끝나면 **원격의 Reduce 워커들이 자기에 할당된 Map의 중간 값들을 네트워크로 가져가서, 로직을 실행해 최종 산출물**을 얻어 낸다.

- 보통 Reduce의 개수는 Map 개수보다 적으며, Map의 중간 데이터 사이즈에 따라 성능이 좌우된다.

#### 모델 적용 적합한 경우

- 분산 Grep이나 빈도 수 계산 등의 작업
    - Map 단계를 거치며 사이즈가 크게 줄어들고, 줄어든 크기만큼 Reduce 오버헤드도 줄어듦 → 성능상 이점이 많다.

#### 모델 적용 부적합한 경우

- 정렬과 같은 작업
    - 입력 데이터 사이즈가 줄어들지 않고 Reduce로 전해지며, 오버헤드에 따라 수행 성능이 저하된다.

### 폴트톨러런스

- **각 프로세스에서는 Master에게 Task 진행 상태를 주기적으로 전송**
- 마스터는 특정 워커의 태스크가 더 이상 진행되지 않거나, 상태 정보를 일정 시간(Heartbeat Timeout) 동안 받지 못하면 문제 발생했다고 결론 내림
- 장애 복구 시 특정 Map이나 Reduce Task들이 죽은 경우, 해당 Task가 처리해야할 데이터 정보만 다른 워커에게 전해주면 워커는 받은 데이터 정보를 인자로 새로운 Task 재실행
- **MapReduce는 Shared Nothing 아키텍처**라 간단한 메커니즘을 가진다.

## 하둡 MapReduce

- 구글이 발표한 논문을 바탕으로 Java 언어로 구현된 시스템

### 아키텍처

|구분                     |설명                                                                                  |
|:-----------------------:|:-------------------------------------------------------------------------------------|
|네임노드(NameNode)       |하둡을 이루는 가장 기본적이고 필수적인 데몬, 네임 스페이스를 관리하는 마스터 역할 수행|
|데이터노드(DataNode)     |분산 파일 시스템의 데몬으로 파일의 실질적 데이터 입출력에 대한 처리 수행              |
|잡트래커(JobTracker)     |MapReduce 시스템에서 Job이라는 작업을 관리하는 마스터 (클러스터에 1개 존재)           |
|태스크트래커(TaskTracker)|작업을 수행하는 워커 데몬이며 슬레이브(각 노드에 1개 존재)                            |

- 하둡은 데몬(서버 메인메모리 상 백그라운드로 수행되는 프로그램) 관점에서 4개의 구성 요소를 가지고 있다.
- **클라이언트에서 잡(Job)이라고 불리는 하둡 작업 실행** 시, 환경 정보들이 JobTracker에게 전송된다.
- JobTracker는 작업을 다수의 **Task(Mapper나 Reduce가 수행하는 단위 작업)**로 나눈다.
    - 데이터 지역성 보장을 위해 어떤 TaskTracker에게 보낼지 감안하여 내부적으로 스케줄링 후 Queue에 저장한다.
- TaskTracker는 JobTracker에게 3초에 한 번씩 주기적으로 하트비트(Heartbeat)를 전송한다.
    - JobTracker는 해당 TaskTracker에게 할당된 Task가 있는지 Queue 확인 후, 있을 경우 하트비트 Response 메세지에 Task 정보를 실어서 TaskTracker에게 보낸다.
    - TaskTracker는 Response 메세지 내용을 통해 프로세스를 fork 후 할당된 Task를 처리한다.

### 실행 절차

1. **스플릿(Split)** : HDFS의 대용량 입력 파일(Input)을 분리하여 파일스플릿을 생성하고, **FileSplit 하나 당 맵 태스크(Map Task) 하나씩 생성**한다.
2. **맵(Map)** : 각 split에 대해 레코드 단위로 Map 함수를 적용하여 **Key-Value 쌍을 생성**한다.
3. **컴바인(Combine)** : 리듀스(Reduce)와 동일한 프로그램을 적용하여, 리듀스(Reduce) 단계로 데이터를 보내기 전에 중간 결과값들을 처리하여 데이터의 크기를 줄여준다.
4. **파티션(Partition)** : Key를 기준으로 데이터를 디스크에 분할 저장하며, 각 파티션은 키를 기준으로 정렬이 수행된다. 또한 분할된 파일들은 각각 다른 리듀스 태스크(Reduce Task)에 저장된다.
5. **셔플(Shuffle)** : 여러 맵퍼들의 결과 파일을 각 리듀서에 ㅎ할당하고, 할당된 파일을 로컬 파일 시스템으로 복사한다.
6. **정렬(Sort)** : 병합 정렬(Merge Sort)방식을 사용하여 맵퍼 결과 파일을 key 기준으로 정렬한다.
7. **리듀스(Reduce)** : 정렬 단계에서 생성된 파일에 대해 리듀스 함수를 적용한다.

- 기본적으로 Output 형태는 Key와 Value를 탭으로 구분하며, mapred.textoutputformat.separator 속성을 사용하여 구분자를 원하는 문자로 변경할 수도 있다.
- **"Word Count"** : 대표적인 맵리듀스 작업
    1. Input
        ```
        DeerBearRiver
        CarCarRiver
        DeerCarBear
        ```
    2. Splitting
        ```
        DeerBearRiver
        -------------
        CarCarRiver
        -------------
        DeerCarBear
        ```
    3. Mapping
        ```
        Deer,1
        Bear,1
        River,1
        -------------
        Car,1
        Car,1
        River,1
        -------------
        Deer,1
        Car,1
        Bear,1
        ```
    4. Shuffling
        ```
        Bear,1
        Bear,1
        -------------
        Car,1
        Car,1
        Car,1
        -------------
        Deer,1
        Deer,1
        -------------
        River,1
        River,1
        ```
    5. Reducing
        ```
        Bear,2
        -------------
        Car,3
        -------------
        Deer,2
        -------------
        River,2
        ```
    6. Output
        ```
        Bear,2
        Car,3
        Deer,2
        River,2
        ```

### 하둡의 성능

- **Sort 작업**은 어떤 작업을 실행하더라도 **Map에서 Reduce로 넘어가는 과정에서 항상 발생하는 내부적인 프로세스**이다.
    - 데이터가 커질수록 **처리 시간이 선형적으로 증가**한다.
- 처리 시간이 줄이고 싶다면, 플랫폼 자체적으로 선형 확장성을 갖고 있어야 한다.
- 이러한 특성 덕분에, Sort는 분산 컴퓨팅 플랫폼 성능과 확장성을 동시에 측정할 수 있는 좋은 실험이다.