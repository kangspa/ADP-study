# MapReduce

- 구글에서 **분산 병렬 컴퓨팅**을 이용해 대용량 데이터 처리를 위해 제작한 소프트웨어 프레임워크 (2004년 "MapReduce:Simplified Data Processing on Large Clusters" 논문으로 공개)
- 분할정복 방식으로 **대용량 데이터를 병렬로 처리**할 수 있는 프로그래밍 모델
    - 분할정복 : 해결하려는 문제를 성질이 같은 여러 부분으로 나누어 해결 후, 원래 문제의 해를 구하는 방식
- C++, Java 등의 언어로 적용 가능하며, 아파치 하둡에서 오픈 소스 프로젝트로 시작한 Java 기반의 "Hadoop MapReduce" 프레임워크가 동일한 기능을 지원
- Client의 수행 작업 단위는 맵리듀스 잡(MapReduce Job)이라고 하며, Job은 Map Task와 Reduce Task로 나뉘어 실행
- 특별한 옵션을 주지 않으면, **Map Task 하나가 1개의 블록(64MB) 대상으로 연산 수행**
    - 해당 과정에서 생성된 중간 결과물들을 **사용자가 지정한 개수에 해당되는 Reduce Task들**이 받아와서 정렬 및 필터링 작업 후 최종 결과물 생성

## 구글 MapReduce

- 대용량 데이터 처리에 발생 가능한 복잡성을 추상화시켜, 핵심 기능 구현에만 집중할 수 있도록 개발

### 프로그래밍 모델 : Map과 Reduce 2개 단계

- **Map에서 Key와 Value의 쌍들을 입력**받는다.
    - 사용자가 정의한 **Map 함수를 거치며 다수의 새로운 Key, Value 쌍들로 변환** 후 로컬 파일 시스템에 임시 저장된다.
- 임시 파일들은 프레임워크에 의해 Reduce에게 전송된다.
    - 해당 과정에서 자동으로 **Shuffling과 Group by 정렬**의 과정을 거친다.
    - **Key와 Value의 리스트 형태**로 **Reduce의 입력** 레코드로 들어간다.
    - 사용자 정의 Reduce 함수를 통해 최종 Output으로 산출된다.
- 사용자 관점에서 세세한 이슈 신경 쓸 필요없이, Map과 Reduce 함수만으로 대규모 병렬 연산 작업 수행이 가능해진다.

### 실행 과정

1. MapReduce 프로그램 작성해서 실행한다.
2. 마스터는 사용자 프로그램에서 지정한 입력 데이터소스를 가지고 스케줄링한다.
3. 하나의 큰 파일을 여러 개의 파일 Split들로 나눈다.
    - **각 Split들이 Map 프로세스들의 할당 단위**
    - **Split 단위는 블록 사이즈인 64MB or 128MB**
4. **Split 수만큼 Map Task들이 워커로부터 Fork되고**, 동시에 실행돼 출력을 로컬 파일시스템에 저장한다.
5. Output 값들은 **Partitioner**라는 Reduce 번호를 할당해 주는 클래스를 통해 어떤 Reduce에게 보내질지 정해진다.
    - 특별히 정해지지 않으면, Key와 해시 값을 Reduce의 개수로 Modular 계산한 값이 부여되어 **동일한 Key들은 같은 Reduce로 배정**된다.
6. Map 단계가 끝나면 **원격의 Reduce 워커들이 자기에 할당된 Map의 중간 값들을 네트워크로 가져가서, 로직을 실행해 최종 산출물**을 얻어 낸다.

- 보통 Reduce의 개수는 Map 개수보다 적으며, Map의 중간 데이터 사이즈에 따라 성능이 좌우된다.

#### 모델 적용 적합한 경우

- 분산 Grep이나 빈도 수 계산 등의 작업
    - Map 단계를 거치며 사이즈가 크게 줄어들고, 줄어든 크기만큼 Reduce 오버헤드도 줄어듦 → 성능상 이점이 많다.

#### 모델 적용 부적합한 경우

- 정렬과 같은 작업
    - 입력 데이터 사이즈가 줄어들지 않고 Reduce로 전해지며, 오버헤드에 따라 수행 성능이 저하된다.

### 폴트톨러런스

- **각 프로세스에서는 Master에게 Task 진행 상태를 주기적으로 전송**
- 마스터는 특정 워커의 태스크가 더 이상 진행되지 않거나, 상태 정보를 일정 시간(Heartbeat Timeout) 동안 받지 못하면 문제 발생했다고 결론 내림
- 장애 복구 시 특정 Map이나 Reduce Task들이 죽은 경우, 해당 Task가 처리해야할 데이터 정보만 다른 워커에게 전해주면 워커는 받은 데이터 정보를 인자로 새로운 Task 재실행
- **MapReduce는 Shared Nothing 아키텍처**라 간단한 메커니즘을 가진다.

## 하둡 MapReduce

- 구글이 발표한 논문을 바탕으로 Java 언어로 구현된 시스템

### 아키텍처

|구분                     |설명                                                                                  |
|:-----------------------:|:-------------------------------------------------------------------------------------|
|네임노드(NameNode)       |하둡을 이루는 가장 기본적이고 필수적인 데몬, 네임 스페이스를 관리하는 마스터 역할 수행|
|데이터노드(DataNode)     |분산 파일 시스템의 데몬으로 파일의 실질적 데이터 입출력에 대한 처리 수행              |
|잡트래커(JobTracker)     |MapReduce 시스템에서 Job이라는 작업을 관리하는 마스터 (클러스터에 1개 존재)           |
|태스크트래커(TaskTracker)|작업을 수행하는 워커 데몬이며 슬레이브(각 노드에 1개 존재)                            |

- 하둡은 데몬(서버 메인메모리 상 백그라운드로 수행되는 프로그램) 관점에서 4개의 구성 요소를 가지고 있다.
- **클라이언트에서 잡(Job)이라고 불리는 하둡 작업 실행** 시, 환경 정보들이 JobTracker에게 전송된다.
- JobTracker는 작업을 다수의 **Task(Mapper나 Reduce가 수행하는 단위 작업)**로 나눈다.
    - 데이터 지역성 보장을 위해 어떤 TaskTracker에게 보낼지 감안하여 내부적으로 스케줄링 후 Queue에 저장한다.
- TaskTracker는 JobTracker에게 3초에 한 번씩 주기적으로 하트비트(Heartbeat)를 전송한다.
    - JobTracker는 해당 TaskTracker에게 할당된 Task가 있는지 Queue 확인 후, 있을 경우 하트비트 Response 메세지에 Task 정보를 실어서 TaskTracker에게 보낸다.
    - TaskTracker는 Response 메세지 내용을 통해 프로세스를 fork 후 할당된 Task를 처리한다.

### 실행 절차

1. **스플릿(Split)** : HDFS의 대용량 입력 파일(Input)을 분리하여 파일스플릿을 생성하고, **FileSplit 하나 당 맵 태스크(Map Task) 하나씩 생성**한다.
2. **맵(Map)** : 각 split에 대해 레코드 단위로 Map 함수를 적용하여 **Key-Value 쌍을 생성**한다.
3. **컴바인(Combine)** : 리듀스(Reduce)와 동일한 프로그램을 적용하여, 리듀스(Reduce) 단계로 데이터를 보내기 전에 중간 결과값들을 처리하여 데이터의 크기를 줄여준다.
4. **파티션(Partition)** : Key를 기준으로 데이터를 디스크에 분할 저장하며, 각 파티션은 키를 기준으로 정렬이 수행된다. 또한 분할된 파일들은 각각 다른 리듀스 태스크(Reduce Task)에 저장된다.
5. **셔플(Shuffle)** : 여러 맵퍼들의 결과 파일을 각 리듀서에 ㅎ할당하고, 할당된 파일을 로컬 파일 시스템으로 복사한다.
6. **정렬(Sort)** : 병합 정렬(Merge Sort)방식을 사용하여 맵퍼 결과 파일을 key 기준으로 정렬한다.
7. **리듀스(Reduce)** : 정렬 단계에서 생성된 파일에 대해 리듀스 함수를 적용한다.

- 기본적으로 Output 형태는 Key와 Value를 탭으로 구분하며, mapred.textoutputformat.separator 속성을 사용하여 구분자를 원하는 문자로 변경할 수도 있다.
- **"Word Count"** : 대표적인 맵리듀스 작업
    1. Input
        ```
        DeerBearRiver
        CarCarRiver
        DeerCarBear
        ```
    2. Splitting
        ```
        DeerBearRiver
        -------------
        CarCarRiver
        -------------
        DeerCarBear
        ```
    3. Mapping
        ```
        Deer,1
        Bear,1
        River,1
        -------------
        Car,1
        Car,1
        River,1
        -------------
        Deer,1
        Car,1
        Bear,1
        ```
    4. Shuffling
        ```
        Bear,1
        Bear,1
        -------------
        Car,1
        Car,1
        Car,1
        -------------
        Deer,1
        Deer,1
        -------------
        River,1
        River,1
        ```
    5. Reducing
        ```
        Bear,2
        -------------
        Car,3
        -------------
        Deer,2
        -------------
        River,2
        ```
    6. Output
        ```
        Bear,2
        Car,3
        Deer,2
        River,2
        ```

### 하둡의 성능

- **Sort 작업**은 어떤 작업을 실행하더라도 **Map에서 Reduce로 넘어가는 과정에서 항상 발생하는 내부적인 프로세스**이다.
    - 데이터가 커질수록 **처리 시간이 선형적으로 증가**한다.
- 처리 시간이 줄이고 싶다면, 플랫폼 자체적으로 선형 확장성을 갖고 있어야 한다.
- 이러한 특성 덕분에, Sort는 분산 컴퓨팅 플랫폼 성능과 확장성을 동시에 측정할 수 있는 좋은 실험이다.

# 병렬 쿼리 시스템

- 일부 사용자들에게는 MapReduce도 새로운 개념이라, 직접 코딩하지 않고도 쉽고 빠르게 서비스(알고리즘)을 구현하고 적용할 수 있는 환경의 필요성이 대두됐다.
- 스크립트나 쿼리 인터페이스를 통해 병렬 처리 할 수 있는 시스템이 개발댔다.
    - 구글 Sawzall, 야후 Pig 등 → 새로운 쿼리 언어로 추상화된 시스템

## 구글 Sawzall

- **MapReduce를 추상화한 최초의 스크립트 형태 병렬 쿼리 언어**
- 사용자가 이해하기 쉬운 인터페이스를 제공하여 개발 생산성을 높임

## 아파치 Pig

- **야후에서 개발**해 오픈소스 프로젝트화한 고차원 언어
- 하둡 MapReduce 위에서 동작하는 추상화된 병렬 처리 언어로, 아파치 하둡 서브 프로젝트에 속함

### 개발 배경

- 대부분의 업무가 한번의 MapReduce 작업으로 끝나지 않는 경우가 많음
- 유사 알고리즘들을 중복 개발하는 경우가 많지만, 공유는 잘 이루어지지 않음

위 요구사항을 해결하기 위해 의미적으로 SQL과 비슷하지만 새로운 언어인 Pig 개발

### MapReduce와 차이

- MapReduce는 무공유 구조라 Join 연산을 매우 복잡하게 처리한다. (약 400라인)
- Pig는 약 10라인의 코드로 해결 가능하여 길이 및 개발 시간에서 많은 이점이 있다.
- 코드 이해도 더 쉽고 직관적이라서 공유가 더 쉽다.

야후 내부의 검색 인프라, 광고 연관성 분석, 사용자 의도 분석, 검색엔진 쿼리 분석, Hoffman's PLSI 등 다양한 분야에서 사용되고 있다.

## 아파치 Hive

- **MapReduce의 모든 기능을 제공한다.**
- 페이스북에서 개발한 데이터 웨어하우징 인프라로 하둡 서브 프로젝트로 등록되어있다.
- Pig처럼 하둡 플랫폼 위에서 동작하며, SQL기반의 쿼리 언어와 JDBC를 지원한다.
- 하둡에서 가장 많이 사용되는 병렬처리 기능, Hadoop-Streaming을 쿼리 내부에 삽입해 사용 가능하다.

### 개발 배경

- 페이스북은 시간이 지나며 운영데이터가 수백 TB 규모로 늘어나며, 라이선스 등 관리 및 운영비용 절감의 필요성을 느낌
- 상용 DBMS를 하둡으로 교체하는 과정에서 필요한 기능들을 하나씩 구현하며 Hive를 완성

### 아키텍처

- MetaStore : Raw File들의 콘텐츠를 일종의 테이블 내 칼럼처럼 구조화된 형태로 관리할 수 있게 해주는 스키마 저장소
- 별도의 DBMS를 설정하지 않으면 **Embedded Derby를 기본 데이터베이스로 사용**
- 사용자는 앞 단의 CLI를 이용해 SQL 쿼리를 사용
    - Parser가 쿼리를 받아 구문 분석 후, MetaStore에 테이블과 파티션 정보 참조하여 Execution Plan을 만든다.
    - Execution Engine은 하둡의 JobTracker와 NameNode와 통신을 담당하는 창구 역할을 하면서 MapReduce 작ㅇ버을 실행하고 파일을 관리한다.
- SerDe : Serializer와 Deserializer의 줄임말
    - 테이블의 로우나 칼럼의 구분자 등 저장 포맷을 정의해주는 컴포넌트
    - 하둡의 InputFormat과 OutputFormat에 해당

### 언어 모델

- DDL (Data Definition Language)
    - 테이블 생성 : Create Table
    - 테이블 삭제 : Drop Table
    - 테이블 변경 : Rename Table
    - 테이블 스키마 변경 : Alter Table, Add Column
    - 테이블 조회 : Show Table
    - 스키마 조회 : Describe Table
- DML (Data Manupulation Language)
    - 로컬에서 DFS로 데이터 로드 : LOAD DATA
    - 쿼리 결과를 테이블이나 로컬 파일 시스템, DFS에 저장
- Query
    - Select
    - Group by
    - Sort by
    - Joins
    - Union
    - Sub Queries
    - Sampling
    - Transform

# SQL on Hadoop

- 하둡의 실시간 처리 제약을 극복하기 위한 **실시간 SQL 질의 분석 기술**
- 대용량 데이터를 대화 형식의 SQL 질의를 통해서 처리하고 분석하는 것

## 임팔라 (Impala)

- SQL on Hadoop 기술 중 가장 먼저 공개된 기술
- **분석과 트랜잭션 처리를 모두 지원**하는 것이 목표
- 하둡과 Hbase에 저장된 데이터 대상으로 SQL 질의 가능
- 고성능을 위해 **C++을 사용**, MapReduce를 사용하지 않고 실행 중 최적화된 코드를 생성해 데이터 처리
- Hive가 하둡에서 비정형 데이터의 표준 SQL 솔루션으로 사용되지만, 더 빠른 처리가 필요한 비즈니스 요구 사항으로 인해 Impala가 대두되는 상황

### 구성요소

- 클라이언트
    - ODBC/JDBC 클라이언트, 임팔라쉘 등에 해당
    - 임팔라에 접속해서 테이블 관리, 데이터 조회 등 작업 수행
- 메타스토어
    - 임팔라로 분석할 대상 데이터들의 정보를 관리
    - 하이브의 메타데이터를 같이 사용
- 임팔라 데몬
    - 시스템 상 ImpalaD로 표시됨
    - 클라이언트이 SQL 질의를 받아서 데이터 파일들의 읽기/쓰기 작업을 수행
    - 질의 실행계획기, 질의 코디네이터, 질의 실행엔진으로 구성됨
- 스테이트 스토어
    - 임팔라 데몬들의 상태 체크 및 건강정보 관리 데몬
    - 임팔라 데몬에 장애 발생 시 다른 데몬들에게 알려 장애가 생긴 데몬에게 질의 요청이 가지 않도록 처리
- 스토리지
    - 분석할 데이터의 저장소
    - 현재는 HBase, HDFS 두 가지 지원

### 동작 방식

- 모든 노드에 임팔라 데몬이 구동되며, 해당 노드에 클라이언트를 이용해 질의 요청할 수 있다.
- 질의는 데이터 지역성을 고려해서 노드 전체로 분산되어 수행된다.
- 요청을 받은 코디네이터 데몬은 각 임팔라 노드들의 부분 결과들을 취합한 후 결과값을 만들어 사용자에게 제공한다.
- 실제 운영 환경에서는 **라운드 로빈 방식**으로 질의를 분산시켜 전 노드들이 코디네이터 역할을 고르게 수행할 수 있도록 한다.
    - 라운드 로빈 : 여러 프로세스들이 순서대로 돌아가며 처리되는 스케줄링 방식

### 임팔라 SQL 구문

기본적으로 하이브 SQL을 이용하지만, 모든 하이브 SQL문을 지원하는 것은 아니다.

- 데이터 정의 언어 (Data Definition Language)
    - 데이터 베이스·테이블 생성 : Create Database/Table
    - 테이블 변경·파티션 추가 : Alter Table
    - 데이터베이스·테이블 삭제 : Drop Database/Table
    - 데이터베이스 테이블 조회 : Show Database/Table, Describe Database
- 데이터 조작 언어 (Data Manipulation Language)
    - 데이터 조회 : Select, Where, GroupBy, OrderBy
    - 데이터 입력 : Insert into, Overwrite
    - 특이사항 : 데이터 변경·삭제 지원 안함 / 테이블 삭제 시 데이터 삭제됨
- 내장 함수 (Builtin Function)
    - 수학함수 : 절대값(abs)/코사인값(acos)/로그값(log) 반환 등의 기능 제공
    - 타입 변환 : 날짜값(day)/유닉스의 포타임(from_unixtime)/현재 시간(now) 반환 등 다수의 함수 제공
    - 조건문 : if문 제공, case 등 분기 기능 제공
    - 문자열 함수 : 아스키 코드값 변환(ascii), 문자열 병합(concat)

### 임팔라 데이터 모델

- 데이터를 HDFS에 주로 저장하며, 저장포맷별 특징은 아래와 같다.

#### row 단위 저장

- 기본 파일 포맷인 텍스트나 시퀀스 파일은 해당 방식으로 주로 저장
- 테이블에서 하나의 column을 읽든, 전체 테이블을 읽든 동일한 디스크 입출력이 발생

#### column 단위 저장

- 읽고자하는 column만큼 디스크 입출력이 발생하기 때문에 처리 성능 개선 가능
    - 전체 column 모두 조회하는 것은 저장 포맷에 따른 성능 영향을 받지 않음
- row 단위 저장보다 처리 시간이 적게 걸리므로 처리 시간의 측면에서 더 효율적
- 처음부터 column 파일 포맷이 아니라면 파일 포맷 변경 작업을 해줘야 한다.
- RCFile(column 단위 저장 포맷)을 사용할 경우, 데이터 처리 과정에서 발생하는 디스크 입출력 양을 현저하게 줄일 수 있다.