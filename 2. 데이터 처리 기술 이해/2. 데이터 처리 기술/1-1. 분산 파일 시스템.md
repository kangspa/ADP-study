# 구글 파일 시스템 (GFS, Google File System)

## 개념 및 특징

- 구글이 자사 사용 목적으로 개발한 분산 파일 시스템
- 파일을 **고정된 크기(64MB)의 청크(Chunk)**들로 나누고, 각 청크에 대한 여러 개의 복제본과 청크를 청크서버에 분산·저장한다.
- 청크 서버들은 데이터를 자동으로 복사하여 저장하고, 주기적으로 청크서버의 상태를 마스터에게 전달한다.
- 청크는 마스터에 의해 생성/삭제될 수 있으며, 유일한 식별자에 의해 구별된다.
- 트리 구조가 아닌 해시 테이블 구조 등을 사용함으로써 메모리 상에서 보다 
효율적인 메타데이터의 처리를 지원한다.

## 가정

- 저가형 서버로 구성된 환경으로, **서버의 고장이 빈번히 발생**한다 가정
- 파일에 대한 **쓰기 연산은 주로 순차적**으로 이루어지며, **갱신은 드묾**
- 낮은 응답 지연시간보다 **높은 처리율이 더 중요**
- 대부분 대용량 파일이라 가정하고, 대용량 파일을 효과적으로 관리하는 방법을 요구
- 작업 부하는 연속적 많은 데이터를 읽거나 임의의 영역에서 적은 데이터를 읽을 때 발생
- 여러 클라이언트에서 동시에 동일한 파일에 데이터 추가하는 환경에서 동기화 오버헤드를 최소화할 수 있는 방법을 요구

## 구성 요소

여러 클라이언트에 의해 접근되는, 하나의 마스터와 청크서버들로 구성된다.

### 클라이언트 (Client)

- 파일에 대한 읽기/쓰기 동작 요청하는 애플리케이션
- POSIX(Portable Operating System Interface)를 지원하지 않으며, 파일 시스템 인터페이스와 유사한 자체 인터페이스를 지원
    - POSIX : 이식 가능 운영 체제 인터페이스
- 여러 클라이언트에서 원자적 데이터 추가 연산을 지원하기 위한 인터페이스 지원

### 마스터 (Master)

- **단일 마스터** 구조로 **모든 메타데이터를 메모리 상에서 관리**
- 청크서버의 하트비트(Heartbeat) 메시지를 주기적으로 수집하여 청크 상태를 회복하는 동작을 수행
- 하나의 청크 서버를 Primary로 지정하여 복제본의 갱신 연산을 일관되게 처리
- 마스터에 대한 장애 처리와 회복을 위해 파일시스템 이름 공간과 파일의 청크 매핑 변경 연산을 로깅하고, 마스터의 상태를 여러 섀도 마스터에 복제

### 청크서버 (Chunk Server)

- 로컬 디스크에 청크를 저장·관리하며 **클라이언트로부터 청크 입출력 요청 처리**
- 하트비트(Heartbeat) 메시지를 통해 청크서버 상태에 대한 정보를 주기적으로 마스터에 전달

## GFS 파일 출력 과정

```
Client ── 요청(파일명) ──► Master
   │                          │
   │◄─ 청크 위치 반환 ────────┘
   │
   └─► Chunk Server에서 직접 데이터 읽기
```

1. 클라이언트가 마스터에 파일 위치 요청
2. 마스터가 클라이언트에 청크 위치(서버의 위치와 핸들) 반환
3. 클라이언트가 직접 청크 서버에 파일 데이터 요청 후 읽기

## GFS 파일 입력 과정
```
Client ── 파일 쓰기 요청 ──► Master
   │                           │
   │◄─ 청크 서버 목록 반환 ────┘
   │
   └─► 1차 Chunk Server ─► 2차 ─► 3차 ... (복제 체인)
```
1. 클라이언트가 마스터에 쓰기 위한 파일(청크) 대해 전달
2. 마스터가 해당 파일에 대한 정보(청크 핸들, 복제될 청크 서버 목록) 반환
3. 클라이언트가 청크 서버에 캐시를 전달하여 청크 서버에 데이터들을 복제

# 하둡 분산 파일 시스템 (HDFS, Hadoop Distributed File System)

## 개념 및 특징

- 아파치 너치(Apache Nutch) 웹 검색 엔진의 파일 시스템으로 개발됨
- 구글 파일 시스템의 아키텍처와 사상을 그대로 구현한 클론 프로젝트
- 대용량 파일을 분산된 서버에 저장하고, 그 데이터를 빠르게 처리할 수 있다.
- 각 구성요소 간 통신을 위해 TCP/IP 네트워크상에서 RPC(Remote Procedure Call)을 사용한다.

## 가정

- **한번 쓰이면 변경되지 않는다** 가정 (2.0 알파버전부터는 append가 가능)
- **순차적 스트리밍 방식**으로 파일을 저장하거나 조회, **배치 작업**에 적합
- 낮은 데이터 접근 지연 시간보다 **높은 데이터 처리량에 중점**

## 구성요소

- **하나의 네임노드(NameNode)**와 **다수의 데이터노드(DataNode)**로 구성되며, 파일 데이터는 블록(또는 청크) 단위로 나뉘어 여러 데이터노드에 분산·복제·저장된다.
- 블록 단위의 경우, 1.0에서는 64MB, 2.0에서는 128MB 단위로 나뉜다.

### 네임노드 (NameNode)

- **마스터 역할**
- **모든 메타데이터 관리**
- 시스템 전반의 상태를 모니터링
- 클라이언트로부터 파일 접근 요청 처리
- 데이터노드들로부터 하트비트를 받아 상태 체크
- 데이터 입출력 요청은 수행하지 않음

### 보조 네임노드

- 상태 모니터링을 보조
- 주기적으로 네임 노드의 파일 시스템 이미지를 스냅샷해서 저장

### 데이터노드(DataNode)

- **슬레이브 역할**
- 클라이언트로부터 데이터 입출력 요청 처리
- 유실 방지를 위해 블록을 **3중 복제**하여 저장
- 블록 저장 시, 해당 블록에 대한 파일의 체크섬(Checksum) 정보를 별도 저장
- 자신의 상태를 나타내는 하트비트(Hearbeat)와 Blockreport(자신이 관리하는 블록 목록)를 네임노드에 전송

## HDFS의 파일 저장 과정
```
Client ── 파일 생성 요청 ──► NameNode
   │                               │
   │◄─ 블록 할당 + DataNode 목록 ──┘
   │
   └─► 1차 DataNode ─► 2차 ─► 3차 ... (복제 체인으로 블록 저장)
```
1. 클라이언트가 네임노드에 파일 생성 정보 전송 및 보관할 노드 목록 요청
2. 네임노드가 블록 할당 및 블록 보관할 데이터 노드 목록 반환
3. 클라이언트가 첫 번째 데이터노드에 데이터 전송 시작
4. 첫 번째 데이터노드는 데이터를 받고, 파이프라인 방식으로 두 번째, 세 번째 데이터노드에 순차 전송
5. 해당 과정을 모든 블록의 저장이 완료될 때까지 반복
6. 모든 복제가 끝나면 데이터노드들이 클라이언트와 네임노드에 성공 응답

## HDFS의 파일 읽기 과정
```
Application
   ▲ ④ 조회 완료된 데이터 전송
   │
   ▼ ① 파일 조회 요청
Client ── ① 파일 정보 요청 ──►  NameNode
   │                                 │
   │◄─ ② 블록 목록 + DataNode 위치 ─┘
   │
   └─► ③ 가장 가까운 DataNode에서 직접 블록 데이터 읽기
```
1. 애플리케이션에서 클라이언트에 파일 조회를 요청하면, 클라이언트는 네임노드에 관련 정보(블록 위치)를 요청한다.
2. 네임노드는 파일에 대한 **모든 블록의 목록과 블록이 저장된 데이터 노드의 위치**를 클라이언트에 반환한다.
3. 클라이언트는 전달받은 블록 위치를 이용해 데이터노드에서 직접 데이터를 읽어온다.
4. 해당 데이터를 애플리케이션에 전송한다.

# 러스터 (Lustre)

## 개념 및 특징

- 클러스터 파일 시스템(Cluster File Systems Inc.)에서 개발한 객체 기반 클러스터 파일 시스템 (Linux와 Cluster의 혼성어)
- 고속 네트워크로 연결된 클라이언트 파일 시스템, 메타데이터 서버, 객체 저장서버들로 구성
- 계층화된 모듈 구조로 TCP/IP, 인피니밴드(Infiniband), 미리넷(Myrinet)과 같은 네트워크를 지원

## 구성요소

### 클라이언트 파일 시스템

- 리눅스 VFS(Virtual File System)에서 설치할 수 있는 파일 시스템
- 메타데이터 서버와 객체 저장 서버들과 통신
- POSIX, 파일 시스템 인터페이스를 제공

### 메타데이터 서버

- 파일 시스템의 이름 공간과 파일에 대한 메타데이터를 관리

### 객체 저장 서버

- 파일데이터를 저장
- 클라이언트로부터 객체 입출력 요청 처리
- **스트라이핑 방식** : 데이터를 세그먼트 단위로 분할 후, 복수의 디스크 장치에 분산시키는 방식

## 구동방식

- 유닉스 시맨틱을 제공하며, 메타데이터에 대해 라이트백 캐시(Write Back Cache) 지원
    - Write Back Cache : 데이터를 캐시에만 저장하고, 어쩔 수 없이 밀릴 경우 하위 저장소에 저장하는 데이터 갱신 방식
- 이를 위해 **클라이언트에서 메타데이터 변경에 대한 갱신 레코드를 우선 생성** 후 메타데이터 서버에 전달
- 메타데이터 서버는 전달된 갱신 레코드를 재수행하여 변경된 메타데이터를 반영
- 파일의 메타데이터와 파일 데이터에 대한 **동시성 제어를 위해 별도의 잠금을 사용**
- 클라이언트와 메타데이터 서버 간 네트워크 트래픽 최소화를 위해, 메타 데이터에 대한 잠금 요청 시 접근 의도를 같이 전달하는 **인텐트(Intent) 기반 잠금 프로토콜 사용**
    - 메타데이터 서버는 접근 의도에 따라 동작 수행과 잠금 승인 처리를 함께 수행하여 트래픽을 줄임
- 메타데이터에 동시 접근이 적을 경우, 클라이언트 캐시를 이용한 라이트백 캐시를 사용
- 메타데이터에 동시 접근이 많을 경우, 클라이언트 캐시를 사용함으로써 발생할 수 있는 오버헤드를 줄임
- 동시 접근 부하에 따라 메타데이터 서버에서 메타데이터를 처리하는 방식도 적용

# GFS vs HDFS vs Lustre

|구분                                         |GFS|HDFS|Lustre|
|---------------------------------------------|---|----|------|
|Open Source                                  | O | O  |  O   |
|Chunk based                                  | O | O  |  X   |
|Support Replication                          | O | O  |  X   |
|Multiple Metadata Server Supported           | X | X  |  X   |
|Locks Used to Maintain Atomicity             | O | O  |  O   |
|Uses a DB for storing Metadata               | X | X  |  X   |
|Adding Nodes without Shutting down the Server| O | O  |  O   |
|POSIX support                                | X | X  |  O   |
|Supports file modification                   | X | X  |  O   |