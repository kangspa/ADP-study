{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b932650d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecd08d16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.9\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d6db25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 선형 계획법 결과 ---\n",
      "최적 생산량 (제품 A, B): [1.33333333 3.33333333]\n",
      "최대 이익: 10.67만원\n",
      "상태: Optimization terminated successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n최적 생산량 (제품 A, B): [1.33333333 3.33333333]\\n최대 이익: 10.67만원\\n상태: Optimization terminated successfully.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import linprog\n",
    "import numpy as np\n",
    "\n",
    "# 예시: 생산 계획 문제\n",
    "# 두 가지 제품 A, B를 생산. 이익을 최대화하는 생산량 결정.\n",
    "# 제품 A: 생산 시간 1시간, 재료 2단위, 이익 3만원\n",
    "# 제품 B: 생산 시간 2시간, 재료 1단위, 이익 2만원\n",
    "# 제약 조건: 총 생산 시간 8시간, 총 재료 6단위\n",
    "\n",
    "# 목적 함수 계수 (최대화 문제이므로 -1을 곱함)\n",
    "# -3 * x_A - 2 * x_B 를 최소화\n",
    "c = np.array([-3, -2])\n",
    "\n",
    "# 부등식 제약 조건 (Ax <= b)\n",
    "# 생산 시간: 1*x_A + 2*x_B <= 8\n",
    "# 재료: 2*x_A + 1*x_B <= 6\n",
    "A = np.array([[1, 2], [2, 1]])\n",
    "b = np.array([8, 6])\n",
    "\n",
    "# 변수 범위 (x_A >= 0, x_B >= 0)\n",
    "x0_bounds = (0, None) # x_A >= 0\n",
    "x1_bounds = (0, None) # x_B >= 0\n",
    "\n",
    "# 선형 계획법 풀이\n",
    "res = linprog(c, A_ub=A, b_ub=b, bounds=[x0_bounds, x1_bounds], method='highs')\n",
    "\n",
    "print(\"--- 선형 계획법 결과 ---\")\n",
    "if res.success:\n",
    "    print(f\"최적 생산량 (제품 A, B): {res.x}\")\n",
    "    print(f\"최대 이익: {-res.fun:.2f}만원\") # -1을 곱했으므로 다시 -1을 곱함\n",
    "else:\n",
    "    print(\"최적해를 찾지 못했습니다.\")\n",
    "print(f\"상태: {res.message}\")\n",
    "'''\n",
    "최적 생산량 (제품 A, B): [1.33333333 3.33333333]\n",
    "최대 이익: 10.67만원\n",
    "상태: Optimization terminated successfully.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfe74ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ADP-study\\.venv\\lib\\site-packages\\pulp\\pulp.py:1316: UserWarning: Spaces are not permitted in the name. Converted to '_'\n",
      "  warnings.warn(\"Spaces are not permitted in the name. Converted to '_'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 정수 선형 계획법 결과 ---\n",
      "상태: Optimal\n",
      "최적 투자 결정:\n",
      "x1 = 1.0\n",
      "x2 = 1.0\n",
      "x3 = 0.0\n",
      "최대 총 수익: 1200.0만원\n"
     ]
    }
   ],
   "source": [
    "from pulp import *\n",
    "\n",
    "# 예시: 투자 선택 문제\n",
    "# 3가지 프로젝트에 투자할지 말지 결정. 총 예산 1000만원.\n",
    "# 프로젝트 1: 비용 400만원, 수익 500만원\n",
    "# 프로젝트 2: 비용 600만원, 수익 700만원\n",
    "# 프로젝트 3: 비용 300만원, 수익 350만원\n",
    "\n",
    "# 문제 정의 (최대화 문제)\n",
    "prob = LpProblem(\"Investment Problem\", LpMaximize)\n",
    "\n",
    "# 결정 변수 정의 (이진 변수: 0 또는 1)\n",
    "# x1, x2, x3는 각 프로젝트에 투자할지 말지를 나타내는 이진 변수\n",
    "x1 = LpVariable(\"x1\", 0, 1, LpBinary)\n",
    "x2 = LpVariable(\"x2\", 0, 1, LpBinary)\n",
    "x3 = LpVariable(\"x3\", 0, 1, LpBinary)\n",
    "\n",
    "# 목적 함수 (총 수익 최대화)\n",
    "prob += 500 * x1 + 700 * x2 + 350 * x3, \"Total Profit\"\n",
    "\n",
    "# 제약 조건 (총 예산 1000만원)\n",
    "prob += 400 * x1 + 600 * x2 + 300 * x3 <= 1000, \"Budget Constraint\"\n",
    "\n",
    "# 문제 풀이\n",
    "prob.solve()\n",
    "\n",
    "print(\"--- 정수 선형 계획법 결과 ---\")\n",
    "print(f\"상태: {LpStatus[prob.status]}\")\n",
    "\n",
    "if LpStatus[prob.status] == \"Optimal\":\n",
    "    print(f\"최적 투자 결정:\")\n",
    "    for v in prob.variables():\n",
    "        print(f\"{v.name} = {v.varValue}\")\n",
    "    print(f\"최대 총 수익: {value(prob.objective)}만원\")\n",
    "else:\n",
    "    print(\"최적해를 찾지 못했습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a4ebc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 혼합 정수 선형 계획법 결과 ---\n",
      "상태: Optimal\n",
      "최적 생산량:\n",
      "x_A = 1.5\n",
      "x_B = 3.0\n",
      "최대 총 이익: 10.5만원\n"
     ]
    }
   ],
   "source": [
    "from pulp import *\n",
    "\n",
    "# 예시: 생산 및 판매 계획 문제\n",
    "# 제품 A, B를 생산하여 판매. 제품 A는 연속적으로 생산 가능, 제품 B는 묶음 단위(정수)로만 생산 가능.\n",
    "# 제품 A: 생산 시간 1시간, 재료 2단위, 이익 3만원\n",
    "# 제품 B: 생산 시간 2시간, 재료 1단위, 이익 2만원\n",
    "# 제약 조건: 총 생산 시간 8시간, 총 재료 6단위\n",
    "\n",
    "# 문제 정의 (최대화 문제)\n",
    "prob = LpProblem(\"Mixed Production Problem\", LpMaximize)\n",
    "\n",
    "# 결정 변수 정의\n",
    "x_A = LpVariable(\"x_A\", 0, None, LpContinuous) # 제품 A 생산량 (연속)\n",
    "x_B = LpVariable(\"x_B\", 0, None, LpInteger)   # 제품 B 생산량 (정수)\n",
    "\n",
    "# 목적 함수 (총 이익 최대화)\n",
    "prob += 3 * x_A + 2 * x_B, \"Total Profit\"\n",
    "\n",
    "# 제약 조건\n",
    "prob += 1 * x_A + 2 * x_B <= 8, \"Time Constraint\" # 생산 시간\n",
    "prob += 2 * x_A + 1 * x_B <= 6, \"Material Constraint\" # 재료\n",
    "\n",
    "# 문제 풀이\n",
    "prob.solve()\n",
    "\n",
    "print(\"--- 혼합 정수 선형 계획법 결과 ---\")\n",
    "print(f\"상태: {LpStatus[prob.status]}\")\n",
    "\n",
    "if LpStatus[prob.status] == \"Optimal\":\n",
    "    print(f\"최적 생산량:\")\n",
    "    for v in prob.variables():\n",
    "        print(f\"{v.name} = {v.varValue}\")\n",
    "    print(f\"최대 총 이익: {value(prob.objective)}만원\")\n",
    "else:\n",
    "    print(\"최적해를 찾지 못했습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a927572",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ADP-study\\.venv\\lib\\site-packages\\torchvision\\models\\_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n",
      "E:\\ADP-study\\.venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\kangs/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# 1. 활성화 함수\n",
    "relu = nn.ReLU()\n",
    "sigmoid = nn.Sigmoid()\n",
    "tanh = nn.Tanh()\n",
    "softmax = nn.Softmax(dim=1)\n",
    "\n",
    "# 2. 손실 함수\n",
    "mse_loss = nn.MSELoss()\n",
    "mae_loss = nn.L1Loss()\n",
    "binary_cross_entropy = nn.BCELoss()\n",
    "categorical_cross_entropy = nn.CrossEntropyLoss() # Softmax 포함\n",
    "\n",
    "# 3. 옵티마이저\n",
    "# model.parameters()는 예시이며, 실제 모델의 파라미터를 전달해야 합니다.\n",
    "# model = YourModel()\n",
    "# optimizer_sgd = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "# optimizer_adam = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# optimizer_rmsprop = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
    "\n",
    "# 4. 규제 (옵티마이저에 weight_decay 파라미터로 L2 규제 적용)\n",
    "# optimizer_adam_l2 = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# 5. Dropout\n",
    "dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "# 6. 파인튜닝 (예: ResNet18 모델 로드 및 수정)\n",
    "# 사전 학습된 ResNet18 모델 로드\n",
    "model_ft = models.resnet18(pretrained=True)\n",
    "\n",
    "# 모든 파라미터를 고정 (가중치 동결)\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 마지막 분류기(fc layer)를 새로운 작업에 맞게 교체\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, 10) # 10개 클래스로 분류하는 작업으로 가정\n",
    "\n",
    "# 교체한 레이어의 파라미터만 학습하도록 설정\n",
    "# optimizer_ft = torch.optim.Adam(model_ft.fc.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0f1f023",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ADP-study\\.venv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "E:\\ADP-study\\.venv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "E:\\ADP-study\\.venv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "E:\\ADP-study\\.venv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "E:\\ADP-study\\.venv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "E:\\ADP-study\\.venv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\ADP-study\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ADP-study\\.venv\\lib\\site-packages\\keras_applications\\resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94658560/94653016 [==============================] - 4s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "# 1. 활성화 함수\n",
    "relu = layers.Activation('relu')\n",
    "sigmoid = layers.Activation('sigmoid')\n",
    "tanh = layers.Activation('tanh')\n",
    "softmax = layers.Activation('softmax')\n",
    "\n",
    "# 2. 손실 함수 (compile 시 문자열로 지정)\n",
    "# model.compile(loss='mean_squared_error')\n",
    "# model.compile(loss='mean_absolute_error')\n",
    "# model.compile(loss='binary_crossentropy')\n",
    "# model.compile(loss='categorical_crossentropy')\n",
    "# model.compile(loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# 3. 옵티마이저 (compile 시 문자열 또는 객체로 지정)\n",
    "# model.compile(optimizer='sgd')\n",
    "# model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001))\n",
    "# model.compile(optimizer='rmsprop')\n",
    "\n",
    "# 4. 규제 (레이어에 kernel_regularizer 인자로 L1/L2 규제 적용)\n",
    "l1_reg = regularizers.l1(0.01)\n",
    "l2_reg = regularizers.l2(0.01)\n",
    "# dense_layer = layers.Dense(64, activation='relu', kernel_regularizer=l2_reg)\n",
    "\n",
    "# 5. Dropout\n",
    "dropout_layer = layers.Dropout(0.5)\n",
    "\n",
    "# 6. 파인튜닝 (예: ResNet50 모델 로드 및 수정)\n",
    "# 사전 학습된 ResNet50 모델 로드 (include_top=False로 분류기 제외)\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# 베이스 모델의 가중치 동결\n",
    "base_model.trainable = False\n",
    "\n",
    "# 새로운 분류기 추가\n",
    "inputs = keras.Input(shape=(224, 224, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "outputs = layers.Dense(10, activation='softmax')(x) # 10개 클래스로 분류\n",
    "model_ft = keras.Model(inputs, outputs)\n",
    "\n",
    "# 새로운 분류기만 학습하도록 컴파일\n",
    "# model_ft.compile(optimizer=keras.optimizers.Adam(),\n",
    "#                  loss='categorical_crossentropy',\n",
    "#                  metrics=['accuracy'])\n",
    "\n",
    "# (선택적) 일부 레이어의 동결 해제 후 미세 조정\n",
    "# base_model.trainable = True\n",
    "# for layer in base_model.layers[:-10]: # 마지막 10개 레이어를 제외하고 동결\n",
    "#     layer.trainable = False\n",
    "#\n",
    "# model_ft.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-5), # 낮은 학습률 사용\n",
    "#                  loss='categorical_crossentropy',\n",
    "#                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "980e73fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\ADP-study\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Keras DNN 이중 분류 정확도: 0.9350\n",
      "WARNING:tensorflow:From E:\\ADP-study\\.venv\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Keras DNN 회귀 MSE: 0.0402\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "\n",
    "# 1. 데이터 준비 (분류 예시)\n",
    "X_clf, y_clf = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=5, random_state=42)\n",
    "X_clf_train, X_clf_test, y_clf_train, y_clf_test = train_test_split(X_clf, y_clf, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_clf_train_scaled = scaler.fit_transform(X_clf_train)\n",
    "X_clf_test_scaled = scaler.transform(X_clf_test)\n",
    "\n",
    "# 2. DNN 모델 구축 (이중 분류)\n",
    "model_dnn_keras = models.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_clf_train_scaled.shape[1],)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid') # 이중 분류\n",
    "])\n",
    "\n",
    "# 3. 모델 컴파일 및 학습\n",
    "model_dnn_keras.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_dnn_keras.fit(X_clf_train_scaled, y_clf_train, epochs=10, batch_size=32, verbose=0)\n",
    "loss, accuracy = model_dnn_keras.evaluate(X_clf_test_scaled, y_clf_test, verbose=0)\n",
    "print(f\"Keras DNN 이중 분류 정확도: {accuracy:.4f}\") # 0.9300\n",
    "\n",
    "# 4. 데이터 준비 (회귀 예시)\n",
    "X_reg, y_reg = make_regression(n_samples=1000, n_features=20, n_informative=10, random_state=42)\n",
    "X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
    "scaler_reg_X = StandardScaler()\n",
    "X_reg_train_scaled = scaler_reg_X.fit_transform(X_reg_train)\n",
    "X_reg_test_scaled = scaler_reg_X.transform(X_reg_test)\n",
    "scaler_reg_y = StandardScaler()\n",
    "y_reg_train_scaled = scaler_reg_y.fit_transform(y_reg_train.reshape(-1, 1))\n",
    "y_reg_test_scaled = scaler_reg_y.transform(y_reg_test.reshape(-1, 1))\n",
    "\n",
    "# 5. DNN 모델 구축 (회귀)\n",
    "model_dnn_reg_keras = models.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_reg_train_scaled.shape[1],)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1, activation='linear') # 회귀\n",
    "])\n",
    "\n",
    "# 6. 모델 컴파일 및 학습\n",
    "model_dnn_reg_keras.compile(optimizer='adam', loss='mse')\n",
    "model_dnn_reg_keras.fit(X_reg_train_scaled, y_reg_train_scaled, epochs=10, batch_size=32, verbose=0)\n",
    "loss_reg = model_dnn_reg_keras.evaluate(X_reg_test_scaled, y_reg_test_scaled, verbose=0)\n",
    "print(f\"Keras DNN 회귀 MSE: {loss_reg:.4f}\") # 0.0315"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8f6186c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch DNN 이중 분류 정확도: 0.9200\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 1. 데이터 준비 (분류 예시)\n",
    "X_clf, y_clf = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=5, random_state=42)\n",
    "X_clf_train, X_clf_test, y_clf_train, y_clf_test = train_test_split(X_clf, y_clf, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_clf_train_scaled = scaler.fit_transform(X_clf_train)\n",
    "X_clf_test_scaled = scaler.transform(X_clf_test)\n",
    "\n",
    "X_clf_train_tensor = torch.tensor(X_clf_train_scaled, dtype=torch.float32)\n",
    "y_clf_train_tensor = torch.tensor(y_clf_train, dtype=torch.float32).reshape(-1, 1)\n",
    "X_clf_test_tensor = torch.tensor(X_clf_test_scaled, dtype=torch.float32)\n",
    "y_clf_test_tensor = torch.tensor(y_clf_test, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "train_dataset_clf = TensorDataset(X_clf_train_tensor, y_clf_train_tensor)\n",
    "train_loader_clf = DataLoader(train_dataset_clf, batch_size=32, shuffle=True)\n",
    "\n",
    "# 2. DNN 모델 구축 (이중 분류)\n",
    "class SimpleDNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleDNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(64, 32)\n",
    "        self.layer3 = nn.Linear(32, 1)\n",
    "        self.sigmoid = nn.Sigmoid() # 이중 분류\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.sigmoid(self.layer3(x))\n",
    "        return x\n",
    "\n",
    "model_dnn_pytorch = SimpleDNN(X_clf_train_scaled.shape[1])\n",
    "\n",
    "# 3. 손실 함수 및 옵티마이저 설정\n",
    "criterion_clf = nn.BCELoss() # 이진 교차 엔트로피\n",
    "optimizer_clf = optim.Adam(model_dnn_pytorch.parameters(), lr=0.001)\n",
    "\n",
    "# 4. 모델 학습\n",
    "for epoch in range(10):\n",
    "    for inputs, labels in train_loader_clf:\n",
    "        optimizer_clf.zero_grad()\n",
    "        outputs = model_dnn_pytorch(inputs)\n",
    "        loss = criterion_clf(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_clf.step()\n",
    "\n",
    "# 5. 모델 평가\n",
    "with torch.no_grad():\n",
    "    y_pred_proba = model_dnn_pytorch(X_clf_test_tensor)\n",
    "    y_pred = (y_pred_proba >= 0.5).float()\n",
    "    accuracy = (y_pred == y_clf_test_tensor).float().mean()\n",
    "    print(f\"PyTorch DNN 이중 분류 정확도: {accuracy.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dbd3d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 1s 0us/step\n",
      "Keras CNN 분류 정확도: 0.9893\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 1. 데이터 로드 및 전처리\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "# 2. CNN 모델 구축\n",
    "model_cnn_keras = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax') # 10개 클래스 분류\n",
    "])\n",
    "\n",
    "# 3. 모델 컴파일 및 학습\n",
    "model_cnn_keras.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_cnn_keras.fit(train_images, train_labels, epochs=5, batch_size=64, verbose=0)\n",
    "loss, accuracy = model_cnn_keras.evaluate(test_images, test_labels, verbose=0)\n",
    "print(f\"Keras CNN 분류 정확도: {accuracy:.4f}\") # 0.9905"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6942f63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "PyTorch CNN 분류 정확도: 98.95%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1. 데이터 로드 및 전처리\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# 2. CNN 모델 구축\n",
    "class SimpleCNN_PyTorch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN_PyTorch, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 10) # MNIST 이미지 크기 28x28 -> 풀링 2번 후 7x7\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7) # Flatten\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "model_cnn_pytorch = SimpleCNN_PyTorch()\n",
    "\n",
    "# 3. 손실 함수 및 옵티마이저 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_cnn_pytorch.parameters(), lr=0.001)\n",
    "\n",
    "# 4. 모델 학습\n",
    "for epoch in range(5):\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_cnn_pytorch(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# 5. 모델 평가\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model_cnn_pytorch(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print(f'PyTorch CNN 분류 정확도: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a25997a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras RNN 예측: 102.41\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "\n",
    "# 1. 데이터 준비 (간단한 시퀀스 예측 예시)\n",
    "# 시퀀스 데이터: [0, 1, 2, 3] -> 4 예측\n",
    "def create_sequences(data, seq_length):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        xs.append(data[i:(i + seq_length)])\n",
    "        ys.append(data[i + seq_length])\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "data = np.arange(0, 100)\n",
    "seq_length = 10\n",
    "X_seq, y_seq = create_sequences(data, seq_length)\n",
    "\n",
    "X_seq = X_seq.reshape(-1, seq_length, 1) # (samples, timesteps, features)\n",
    "\n",
    "# 2. RNN 모델 구축\n",
    "model_rnn_keras = models.Sequential([\n",
    "    layers.SimpleRNN(32, activation='relu', input_shape=(seq_length, 1)),\n",
    "    layers.Dense(1) # 회귀 예측\n",
    "])\n",
    "\n",
    "# 3. 모델 컴파일 및 학습\n",
    "model_rnn_keras.compile(optimizer='adam', loss='mse')\n",
    "model_rnn_keras.fit(X_seq, y_seq, epochs=10, verbose=0)\n",
    "\n",
    "# 4. 예측\n",
    "new_sequence = np.array([90, 91, 92, 93, 94, 95, 96, 97, 98, 99]).reshape(1, seq_length, 1)\n",
    "predicted_value = model_rnn_keras.predict(new_sequence)[0][0]\n",
    "print(f\"Keras RNN 예측: {predicted_value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb2e1399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch RNN 예측: 4.43\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# 1. 데이터 준비 (간단한 시퀀스 예측 예시)\n",
    "def create_sequences_torch(data, seq_length):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        xs.append(data[i:(i + seq_length)])\n",
    "        ys.append(data[i + seq_length])\n",
    "    return torch.tensor(np.array(xs), dtype=torch.float32).unsqueeze(-1), \\\n",
    "           torch.tensor(np.array(ys), dtype=torch.float32).unsqueeze(-1)\n",
    "\n",
    "data = np.arange(0, 100)\n",
    "seq_length = 10\n",
    "X_seq_torch, y_seq_torch = create_sequences_torch(data, seq_length)\n",
    "\n",
    "# 2. RNN 모델 구축\n",
    "class SimpleRNN_PyTorch(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNN_PyTorch, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device) # (num_layers * num_directions, batch, hidden_size)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :]) # 마지막 시점의 은닉 상태 사용\n",
    "        return out\n",
    "\n",
    "model_rnn_pytorch = SimpleRNN_PyTorch(input_size=1, hidden_size=32, output_size=1)\n",
    "\n",
    "# 3. 손실 함수 및 옵티마이저 설정\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model_rnn_pytorch.parameters(), lr=0.01)\n",
    "\n",
    "# 4. 모델 학습\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model_rnn_pytorch(X_seq_torch)\n",
    "    loss = criterion(outputs, y_seq_torch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# 5. 예측\n",
    "new_sequence_torch = torch.tensor(np.array([90, 91, 92, 93, 94, 95, 96, 97, 98, 99]), dtype=torch.float32).reshape(1, seq_length, 1)\n",
    "with torch.no_grad():\n",
    "    predicted_value = model_rnn_pytorch(new_sequence_torch).item()\n",
    "print(f\"PyTorch RNN 예측: {predicted_value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6901382f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras LSTM 예측: 104.18\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "\n",
    "# 1. 데이터 준비 (RNN 예시와 동일)\n",
    "def create_sequences(data, seq_length):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        xs.append(data[i:(i + seq_length)])\n",
    "        ys.append(data[i + seq_length])\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "data = np.arange(0, 100)\n",
    "seq_length = 10\n",
    "X_seq, y_seq = create_sequences(data, seq_length)\n",
    "\n",
    "X_seq = X_seq.reshape(-1, seq_length, 1) # (samples, timesteps, features)\n",
    "\n",
    "# 2. LSTM 모델 구축\n",
    "model_lstm_keras = models.Sequential([\n",
    "    layers.LSTM(32, activation='relu', input_shape=(seq_length, 1)),\n",
    "    layers.Dense(1) # 회귀 예측\n",
    "])\n",
    "\n",
    "# 3. 모델 컴파일 및 학습\n",
    "model_lstm_keras.compile(optimizer='adam', loss='mse')\n",
    "model_lstm_keras.fit(X_seq, y_seq, epochs=10, verbose=0)\n",
    "\n",
    "# 4. 예측\n",
    "new_sequence = np.array([90, 91, 92, 93, 94, 95, 96, 97, 98, 99]).reshape(1, seq_length, 1)\n",
    "predicted_value = model_lstm_keras.predict(new_sequence)[0][0]\n",
    "print(f\"Keras LSTM 예측: {predicted_value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1369e362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch LSTM 예측: 1.91\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# 1. 데이터 준비 (RNN 예시와 동일)\n",
    "def create_sequences_torch(data, seq_length):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        xs.append(data[i:(i + seq_length)])\n",
    "        ys.append(data[i + seq_length])\n",
    "    return torch.tensor(np.array(xs), dtype=torch.float32).unsqueeze(-1), \\\n",
    "           torch.tensor(np.array(ys), dtype=torch.float32).unsqueeze(-1)\n",
    "\n",
    "data = np.arange(0, 100)\n",
    "seq_length = 10\n",
    "X_seq_torch, y_seq_torch = create_sequences_torch(data, seq_length)\n",
    "\n",
    "# 2. LSTM 모델 구축\n",
    "class SimpleLSTM_PyTorch(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleLSTM_PyTorch, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :]) # 마지막 시점의 은닉 상태 사용\n",
    "        return out\n",
    "\n",
    "model_lstm_pytorch = SimpleLSTM_PyTorch(input_size=1, hidden_size=32, output_size=1)\n",
    "\n",
    "# 3. 손실 함수 및 옵티마이저 설정\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model_lstm_pytorch.parameters(), lr=0.01)\n",
    "\n",
    "# 4. 모델 학습\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model_lstm_pytorch(X_seq_torch)\n",
    "    loss = criterion(outputs, y_seq_torch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# 5. 예측\n",
    "new_sequence_torch = torch.tensor(np.array([90, 91, 92, 93, 94, 95, 96, 97, 98, 99]), dtype=torch.float32).reshape(1, seq_length, 1)\n",
    "with torch.no_grad():\n",
    "    predicted_value = model_lstm_pytorch(new_sequence_torch).item()\n",
    "print(f\"PyTorch LSTM 예측: {predicted_value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9808befc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7502fe35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a26d02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96575991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e288d931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83539ca5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40902b5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e7977a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b906c3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f754bc17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67113663",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd57057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807e8f71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891be021",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff9da44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9457862",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255fa604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c7a23d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8125ffa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746c0168",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda5a4d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
