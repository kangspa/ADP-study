{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f51188b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "888de99c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.12\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "130b1ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.23.2'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a02095d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# 텍스트 데이터 준비\n",
    "documents = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Never jump over the lazy dog.\",\n",
    "    \"The dog is very lazy.\",\n",
    "    \"A quick brown fox is quick.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db026de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CountVectorizer 결과 ---\n",
      "단어 사전 (Vocabulary): ['brown', 'dog', 'fox', 'jump', 'jumps', 'lazy', 'quick']\n",
      "변환된 행렬 (Sparse Matrix):\n",
      " [[1 1 1 0 1 1 1]\n",
      " [0 1 0 1 0 1 0]\n",
      " [0 1 0 0 0 1 0]\n",
      " [1 0 1 0 0 0 2]]\n",
      "변환된 행렬 shape: (4, 7)\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words='english', ngram_range=(1, 1))\n",
    "count_matrix = count_vectorizer.fit_transform(documents)\n",
    "\n",
    "print(\"--- CountVectorizer 결과 ---\")\n",
    "print(\"단어 사전 (Vocabulary):\", count_vectorizer.get_feature_names())\n",
    "print(\"변환된 행렬 (Sparse Matrix):\\n\", count_matrix.toarray())\n",
    "print(\"변환된 행렬 shape:\", count_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bdc0625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TfidfVectorizer 결과 ---\n",
      "단어 사전 (Vocabulary): ['brown', 'dog', 'fox', 'jump', 'jumps', 'lazy', 'quick']\n",
      "변환된 행렬 (Sparse Matrix):\n",
      " [[0.41101031 0.33274827 0.41101031 0.         0.52131446 0.33274827\n",
      "  0.41101031]\n",
      " [0.         0.47380449 0.         0.74230628 0.         0.47380449\n",
      "  0.        ]\n",
      " [0.         0.70710678 0.         0.         0.         0.70710678\n",
      "  0.        ]\n",
      " [0.40824829 0.         0.40824829 0.         0.         0.\n",
      "  0.81649658]]\n",
      "변환된 행렬 shape: (4, 7)\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 1))\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "print(\"\\n--- TfidfVectorizer 결과 ---\")\n",
    "print(\"단어 사전 (Vocabulary):\", tfidf_vectorizer.get_feature_names())\n",
    "print(\"변환된 행렬 (Sparse Matrix):\\n\", tfidf_matrix.toarray())\n",
    "print(\"변환된 행렬 shape:\", tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e3a88eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.31531524]]\n",
      "=== Cosine Similarity ===\n",
      " [[1.    0.315 0.471 0.671]\n",
      " [0.315 1.    0.67  0.   ]\n",
      " [0.471 0.67  1.    0.   ]\n",
      " [0.671 0.    0.    1.   ]]\n",
      "\n",
      "=== Euclidean Similarity ===\n",
      " [[1.    0.461 0.493 0.552]\n",
      " [0.461 1.    0.552 0.414]\n",
      " [0.493 0.552 1.    0.414]\n",
      " [0.552 0.414 0.414 1.   ]]\n",
      "\n",
      "=== Manhattan Similarity ===\n",
      " [[1.    0.265 0.285 0.385]\n",
      " [0.265 1.    0.453 0.231]\n",
      " [0.285 0.453 1.    0.247]\n",
      " [0.385 0.231 0.247 1.   ]]\n",
      "\n",
      "=== Jaccard Similarity ===\n",
      " [[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, manhattan_distances\n",
    "from scipy.spatial.distance import jaccard\n",
    "import numpy as np\n",
    "\n",
    "# 코사인 유사도 (기본)\n",
    "cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "# 개별 비교 시 아래처럼 작성 가능 ([[0.31531524]])\n",
    "print(cosine_similarity(tfidf_matrix[0], tfidf_matrix[1]))\n",
    "\n",
    "# 유클리디언 거리 → 유사도로 변환 (1 / (1 + 거리))\n",
    "euclidean_dist = euclidean_distances(tfidf_matrix)\n",
    "euclidean_sim = 1 / (1 + euclidean_dist)\n",
    "\n",
    "# 맨하탄 거리 → 유사도로 변환\n",
    "manhattan_dist = manhattan_distances(tfidf_matrix)\n",
    "manhattan_sim = 1 / (1 + manhattan_dist)\n",
    "\n",
    "# 자카드 유사도 (TF-IDF는 연속값이므로 주로 binary로 변환 후 사용)\n",
    "binary_matrix = (tfidf_matrix > 0).astype(int)\n",
    "n_docs = len(documents)\n",
    "jaccard_sim = np.zeros((n_docs, n_docs))\n",
    "\n",
    "for i in range(n_docs):\n",
    "    for j in range(n_docs):\n",
    "        jaccard_sim[i, j] = 1 - jaccard(binary_matrix[i], binary_matrix[j])  # 1 - 거리 = 유사도\n",
    "\n",
    "# 결과 출력\n",
    "print(\"=== Cosine Similarity ===\\n\", np.round(cosine_sim, 3))\n",
    "'''\n",
    "[[1.    0.315 0.471 0.671]\n",
    " [0.315 1.    0.67  0.   ]\n",
    " [0.471 0.67  1.    0.   ]\n",
    " [0.671 0.    0.    1.   ]]\n",
    "'''\n",
    "print(\"\\n=== Euclidean Similarity ===\\n\", np.round(euclidean_sim, 3))\n",
    "'''\n",
    "[[1.    0.461 0.493 0.552]\n",
    " [0.461 1.    0.552 0.414]\n",
    " [0.493 0.552 1.    0.414]\n",
    " [0.552 0.414 0.414 1.   ]]\n",
    "'''\n",
    "print(\"\\n=== Manhattan Similarity ===\\n\", np.round(manhattan_sim, 3))\n",
    "'''\n",
    "[[1.    0.265 0.285 0.385]\n",
    " [0.265 1.    0.453 0.231]\n",
    " [0.285 0.453 1.    0.247]\n",
    " [0.385 0.231 0.247 1.   ]]\n",
    "'''\n",
    "print(\"\\n=== Jaccard Similarity ===\\n\", np.round(jaccard_sim, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7952a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 감성 분석 모델 평가 ---\n",
      "정확도: 0.333\n",
      "분류 리포트:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      1.00      0.50         1\n",
      "           1       0.00      0.00      0.00         1\n",
      "           2       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.33         3\n",
      "   macro avg       0.11      0.33      0.17         3\n",
      "weighted avg       0.11      0.33      0.17         3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kangs\\miniconda3\\envs\\ADP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 1. 데이터 준비 (예시: 간단한 감성 데이터)\n",
    "# 실제 데이터는 더 많은 양과 다양한 감성 표현을 포함해야 함\n",
    "texts = [\n",
    "    \"이 영화 정말 최고다! 강력 추천합니다.\", # 긍정\n",
    "    \"시간 낭비였어요. 너무 지루하고 재미없네요.\", # 부정\n",
    "    \"그냥 볼만했어요. 특별히 좋지도 나쁘지도 않아요.\", # 중립\n",
    "    \"인생 영화 등극! 다시 보고 싶어요.\", # 긍정\n",
    "    \"최악의 경험. 돈 아까워요.\", # 부정\n",
    "    \"나쁘지 않은데, 기대했던 것보다는 별로였어요.\", # 부정 (중립에 가까움)\n",
    "    \"딱 평범하게 볼만한 것 같아요.\" # 중립\n",
    "]\n",
    "labels = [1, 0, 2, 1, 0, 0, 2] # 1: 긍정, 0: 부정, 2: 중립\n",
    "\n",
    "df_sentiment = pd.DataFrame({'text': texts, 'sentiment': labels})\n",
    "\n",
    "# 2. 텍스트 전처리 (간단화) 및 벡터화\n",
    "# 실제로는 토큰화, 불용어 제거 등 더 복잡한 전처리 필요\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=100)\n",
    "X_vec = vectorizer.fit_transform(df_sentiment['text'])\n",
    "y_sent = df_sentiment['sentiment']\n",
    "\n",
    "X_train_sent, X_test_sent, y_train_sent, y_test_sent = train_test_split(X_vec, y_sent, test_size=0.3, random_state=42, stratify=y_sent)\n",
    "\n",
    "# 3. 모델 학습 (로지스틱 회귀)\n",
    "sentiment_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "sentiment_model.fit(X_train_sent, y_train_sent)\n",
    "\n",
    "# 4. 예측 및 평가\n",
    "y_pred_sent = sentiment_model.predict(X_test_sent)\n",
    "print(\"--- 감성 분석 모델 평가 ---\")\n",
    "print(f\"정확도: {accuracy_score(y_test_sent, y_pred_sent):.3f}\")\n",
    "print(\"분류 리포트:\\n\", classification_report(y_test_sent, y_pred_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5476a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- LDA 토픽 확인 ---\n",
      "Topic 0: artificial (2.48) + field (2.48) + intelligence (2.48) + learning (1.95) + deep (1.50) + computer (1.50) + vision (1.50) + neural (1.49) + networks (1.49) + used (1.49)\n",
      "Topic 1: learning (5.05) + machine (4.26) + natural (1.50) + techniques (1.50) + processing (1.50) + uses (1.50) + language (1.50) + deep (1.50) + combines (1.50) + statistics (1.50)\n",
      "\n",
      "--- 문서별 토픽 분포 ---\n",
      "Document 0: [(0, 0.698), (1, 0.302)]\n",
      "Document 1: [(0, 0.1), (1, 0.9)]\n",
      "Document 2: [(0, 0.066), (1, 0.934)]\n",
      "Document 3: [(0, 0.915), (1, 0.085)]\n",
      "Document 4: [(0, 0.076), (1, 0.924)]\n",
      "Document 5: [(0, 0.883), (1, 0.117)]\n",
      "\n",
      "Approx. Coherence (mean dominant topic prob): 0.876\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 1. 텍스트 데이터 준비\n",
    "documents_lda = [\n",
    "    \"machine learning is a field of artificial intelligence\",\n",
    "    \"deep learning is a subset of machine learning\",\n",
    "    \"natural language processing uses machine learning techniques\",\n",
    "    \"computer vision is another field of artificial intelligence\",\n",
    "    \"data science combines statistics and machine learning\",\n",
    "    \"neural networks are used in deep learning\"\n",
    "]\n",
    "\n",
    "# 2. 벡터화 (CountVectorizer)\n",
    "# - TF-IDF가 아니라 단어 빈도를 써야 LDA에서 확률 해석이 가능\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(documents_lda)\n",
    "\n",
    "# 3. LDA 모델 학습\n",
    "# n_components: 토픽 개수\n",
    "lda_model = LatentDirichletAllocation(\n",
    "    n_components=2,\n",
    "    random_state=42,\n",
    "    learning_method='batch',\n",
    "    max_iter=20\n",
    ")\n",
    "lda_model.fit(X)\n",
    "\n",
    "# 4. 단어 사전 (피처 이름)\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "# 5. 토픽별 주요 단어 출력\n",
    "def print_topics(model, feature_names, n_top_words=10):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx}: \", end=\"\")\n",
    "        top_features = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        print(\" + \".join([f\"{feature_names[i]} ({topic[i]:.2f})\" for i in top_features]))\n",
    "\n",
    "print(\"--- LDA 토픽 확인 ---\")\n",
    "print_topics(lda_model, terms, n_top_words=10)\n",
    "'''\n",
    "Topic 0: artificial (2.48) + field (2.48) + intelligence (2.48) + learning (1.95) + deep (1.50) + computer (1.50) + vision (1.50) + neural (1.49) + networks (1.49) + used (1.49)\n",
    "Topic 1: learning (5.05) + machine (4.26) + natural (1.50) + techniques (1.50) + processing (1.50) + uses (1.50) + language (1.50) + deep (1.50) + combines (1.50) + statistics (1.50)\n",
    "'''\n",
    "\n",
    "# 6. 문서별 토픽 분포 확인\n",
    "doc_topic_distr = lda_model.transform(X)\n",
    "\n",
    "print(\"\\n--- 문서별 토픽 분포 ---\")\n",
    "for i, topic_probs in enumerate(doc_topic_distr):\n",
    "    print(f\"Document {i}: {[(j, round(p, 3)) for j, p in enumerate(topic_probs)]}\")\n",
    "'''\n",
    "Document 0: [(0, 0.698), (1, 0.302)]\n",
    "Document 1: [(0, 0.1), (1, 0.9)]\n",
    "Document 2: [(0, 0.066), (1, 0.934)]\n",
    "Document 3: [(0, 0.915), (1, 0.085)]\n",
    "Document 4: [(0, 0.076), (1, 0.924)]\n",
    "Document 5: [(0, 0.883), (1, 0.117)]\n",
    "'''\n",
    "\n",
    "# 7. 토픽 일관성(Coherence) 유사 지표로 대체\n",
    "# scikit-learn에는 gensim의 CoherenceModel이 없음.\n",
    "# 대신 각 문서의 가장 높은 토픽 확률 평균을 간단한 \"일관성\" 근사값으로 사용 가능.\n",
    "coherence_like = np.mean(np.max(doc_topic_distr, axis=1))\n",
    "print(f\"\\nApprox. Coherence (mean dominant topic prob): {coherence_like:.3f}\") # 0.876"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30f69722",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kangs\\miniconda3\\envs\\ADP\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\kangs\\miniconda3\\envs\\ADP\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\kangs\\miniconda3\\envs\\ADP\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\kangs\\miniconda3\\envs\\ADP\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\kangs\\miniconda3\\envs\\ADP\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\kangs\\miniconda3\\envs\\ADP\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kangs\\miniconda3\\envs\\ADP\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\kangs\\miniconda3\\envs\\ADP\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 10, 100)           3400      \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d (SpatialDr (None, 10, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 83,901\n",
      "Trainable params: 83,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\kangs\\miniconda3\\envs\\ADP\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "\n",
      "--- LSTM 텍스트 분류 모델 평가 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67         1\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kangs\\miniconda3\\envs\\ADP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 1. 텍스트 데이터 준비 (예시)\n",
    "texts = [\n",
    "    \"I love this movie, it's amazing!\",\n",
    "    \"This film was terrible, a complete waste of time.\",\n",
    "    \"The acting was good but the plot was boring.\",\n",
    "    \"Highly recommend this, a must-watch.\",\n",
    "    \"Never watch this again, so bad.\",\n",
    "    \"It was okay, not great not terrible.\"\n",
    "]\n",
    "labels = np.array([1, 0, 0, 1, 0, 0]) # 1: 긍정, 0: 부정\n",
    "\n",
    "# 2. 텍스트 전처리 및 벡터화 (토큰화 및 시퀀스 패딩)\n",
    "# num_words: 사용할 단어의 최대 개수\n",
    "tokenizer = Tokenizer(num_words=1000, oov_token=\"<unk>\")\n",
    "tokenizer.fit_on_texts(texts)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# 텍스트를 시퀀스로 변환\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# 시퀀스 패딩 (길이 맞추기)\n",
    "# maxlen: 시퀀스의 최대 길이\n",
    "# padding: 'pre' (앞에 0 채우기) 또는 'post' (뒤에 0 채우기)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=10, padding='post')\n",
    "\n",
    "# 3. 훈련/테스트 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.3, random_state=42, stratify=labels)\n",
    "\n",
    "# 4. LSTM 모델 구축\n",
    "# vocab_size: 단어 사전의 크기\n",
    "# embedding_dim: 임베딩 벡터의 차원\n",
    "# input_length: 입력 시퀀스의 길이 (maxlen과 동일)\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=len(word_index) + 1, output_dim=100, input_length=10),\n",
    "    SpatialDropout1D(0.2), # 과적합 방지\n",
    "    LSTM(100, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(1, activation='sigmoid') # 이진 분류이므로 sigmoid\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "'''\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "embedding (Embedding)        (None, 10, 100)           3400      \n",
    "_________________________________________________________________\n",
    "spatial_dropout1d (SpatialDr (None, 10, 100)           0         \n",
    "_________________________________________________________________\n",
    "lstm (LSTM)                  (None, 100)               80400     \n",
    "_________________________________________________________________\n",
    "dense (Dense)                (None, 1)                 101       \n",
    "=================================================================\n",
    "Total params: 83,901\n",
    "Trainable params: 83,901\n",
    "Non-trainable params: 0\n",
    "'''\n",
    "\n",
    "# 5. 모델 학습\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "# 6. 예측 및 평가\n",
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "print(\"\\n--- LSTM 텍스트 분류 모델 평가 ---\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400accd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5465ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bc3474",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6594700a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000ca4ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e651a56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd448908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0402656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2289cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ab0cc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a424ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f3720a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974fba75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b455d658",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
