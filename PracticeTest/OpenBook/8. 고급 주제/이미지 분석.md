# 이미지 분석

## 개념 요약

이미지 분석(Image Analysis)은 디지털 이미지로부터 의미 있는 정보나 패턴을 추출하고 해석하는 기술입니다. 이는 컴퓨터 비전(Computer Vision) 분야의 핵심 요소로, 이미지 내의 객체를 식별하거나 분류하고, 특정 영역을 분할하며, 이미지의 내용을 이해하는 것을 목표로 합니다. 최근에는 컨볼루션 신경망(Convolutional Neural Network, CNN)을 기반으로 한 딥러닝 기술이 이미지 분석 분야에서 혁혁한 성과를 거두고 있습니다.

## 적용 가능한 상황

- **의료 영상 분석**: X-ray, MRI, CT 이미지에서 질병 진단(종양 탐지, 병변 분류) 및 의료 영상 분할.
- **자율 주행**: 도로, 차량, 보행자, 신호등 등 주변 환경 객체 인식 및 차선 감지.
- **품질 관리**: 제조 공정에서 제품의 결함 탐지 및 불량품 분류.
- **보안 및 감시**: 얼굴 인식, 객체 추적, 이상 행동 감지.
- **농업**: 작물 질병 진단, 수확량 예측, 잡초 탐지.
- **소매**: 고객 행동 분석, 상품 인식, 재고 관리.

## 이미지 데이터 전처리 및 증강 방식 정리

### 1. 이미지 데이터 전처리

#### 용도

이미지 전처리는 원본 이미지를 모델이 학습하기에 적합한 형태로 변환하고, 불필요한 노이즈를 제거하거나 특정 특징을 강조하여 모델의 학습 효율성과 성능을 향상시키는 과정입니다.

#### 주의사항

- 과도한 전처리는 원본 이미지의 중요한 정보를 손실시키거나 왜곡시킬 수 있습니다.
- 모델의 종류나 데이터의 특성에 따라 적절한 전처리 기법을 선택해야 합니다.

#### 방법

- **크기 조정 (Resizing)**: 모든 이미지를 모델 입력에 맞는 동일한 크기로 조정합니다. `cv2.resize` (OpenCV) 또는 `PIL.Image.resize` (Pillow)를 주로 사용합니다.
- **정규화 (Normalization)**: 픽셀 값을 특정 범위(예: 0~1 또는 -1~1)로 조정하여 모델 학습의 안정성을 높입니다. 일반적으로 픽셀 값을 255로 나누어 0~1 범위로 만듭니다.
- **평균/표준편차 정규화**: 데이터셋의 평균과 표준편차를 이용하여 픽셀 값을 정규 분포에 가깝게 만듭니다. `torchvision.transforms.Normalize`에서 `mean`과 `std` 인자를 사용합니다.
- **그레이스케일 변환 (Grayscale Conversion)**: 컬러 이미지를 단일 채널의 흑백 이미지로 변환합니다. 채널 수를 줄여 계산량을 감소시킬 수 있습니다. `cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)` 또는 `PIL.Image.convert('L')`을 사용합니다.
- **노이즈 제거 (Noise Reduction)**: 가우시안 블러(Gaussian Blur), 미디언 필터(Median Filter) 등을 사용하여 이미지의 노이즈를 줄입니다. `cv2.GaussianBlur`, `cv2.medianBlur`.

#### 코드 예시 (Python)

```python
import cv2
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt

# 예시 이미지 생성 (실제 사용 시에는 이미지 파일을 불러와야 함)
# 100x100 크기의 컬러 이미지 (BGR 형식)
img_bgr = np.random.randint(0, 256, (100, 100, 3), dtype=np.uint8)

# 1. 크기 조정 (Resizing)
resized_img = cv2.resize(img_bgr, (64, 64)) # 64x64 크기로 조정

# 2. 정규화 (Normalization) - 0-1 범위
normalized_img = resized_img.astype(np.float32) / 255.0

# 3. 그레이스케일 변환
grayscale_img = cv2.cvtColor(resized_img, cv2.COLOR_BGR2GRAY)

# 4. 노이즈 추가 및 제거 (예시를 위해 가우시안 노이즈 추가 후 제거)
noisy_img = resized_img + np.random.normal(0, 20, resized_img.shape).astype(np.uint8)
denoisy_img = cv2.GaussianBlur(noisy_img, (5, 5), 0) # 가우시안 블러 적용

# 시각화
plt.figure(figsize=(12, 4))
plt.subplot(1, 4, 1)
plt.title("Original (BGR)")
plt.imshow(cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)) # matplotlib은 RGB를 사용
plt.axis('off')

plt.subplot(1, 4, 2)
plt.title("Resized & Normalized (0-1)")
plt.imshow(normalized_img) # 이미 정규화된 이미지는 0-1 범위이므로 그대로 표시
plt.axis('off')

plt.subplot(1, 4, 3)
plt.title("Grayscale")
plt.imshow(grayscale_img, cmap='gray')
plt.axis('off')

plt.subplot(1, 4, 4)
plt.title("Denoised")
plt.imshow(cv2.cvtColor(denoisy_img, cv2.COLOR_BGR2RGB))
plt.axis('off')

plt.tight_layout()
plt.show()
```

### 2. 이미지 데이터 증강 (Augmentation)

#### 용도

이미지 증강은 기존 데이터셋의 이미지를 인위적으로 변형하여 데이터셋의 크기를 늘리는 기법입니다. 이는 모델의 과적합을 방지하고, 다양한 환경 변화에 강인한(robust) 모델을 구축하여 일반화 성능을 향상시키는 데 매우 효과적입니다.

#### 주의사항

- 증강 기법은 도메인 지식과 모델의 특성을 고려하여 적절하게 선택해야 합니다. 예를 들어, 의료 영상에서는 좌우 반전이 의미가 있을 수 있지만, 상하 반전은 의미가 없을 수 있습니다.
- 과도한 증강은 이미지의 본질적인 특징을 손상시켜 모델 학습에 방해가 될 수 있습니다.

#### 방법

- **기하학적 변환**: 이미지의 공간적 특성을 변경합니다.
    - **좌우/상하 반전 (Flip)**: `torchvision.transforms.RandomHorizontalFlip`, `RandomVerticalFlip`, `tf.keras.preprocessing.image.ImageDataGenerator(horizontal_flip=True)`
    - **회전 (Rotation)**: `torchvision.transforms.RandomRotation`, `ImageDataGenerator(rotation_range=...)`
    - **이동 (Translation)**: `torchvision.transforms.RandomAffine`, `ImageDataGenerator(width_shift_range=..., height_shift_range=...)`
    - **확대/축소 (Zoom)**: `torchvision.transforms.RandomResizedCrop`, `ImageDataGenerator(zoom_range=...)`
- **색상 변환**: 이미지의 색상 특성을 변경합니다.
    - **밝기/대비/채도 조절 (Brightness/Contrast/Saturation Adjustment)**: `torchvision.transforms.ColorJitter`, `ImageDataGenerator(brightness_range=...)`
- **노이즈 추가**: 이미지에 인위적인 노이즈를 추가하여 모델의 강인성을 높입니다.
- **Cutout, Mixup, CutMix**: 이미지의 일부를 가리거나 여러 이미지를 혼합하여 새로운 이미지를 생성하는 고급 증강 기법입니다.

#### 코드 예시 (PyTorch `torchvision.transforms`)

```python
import torch
from torchvision import transforms
from PIL import Image
import matplotlib.pyplot as plt

# 예시 이미지 생성 (실제 사용 시에는 이미지 파일을 불러와야 함)
# PIL Image 객체로 변환
img_pil = Image.fromarray(np.random.randint(0, 256, (128, 128, 3), dtype=np.uint8))

# 증강 파이프라인 정의
augment_transform = transforms.Compose([
    transforms.RandomResizedCrop(128), # 랜덤 크롭 후 리사이즈
    transforms.RandomHorizontalFlip(), # 50% 확률로 좌우 반전
    transforms.RandomRotation(15),    # -15도 ~ +15도 랜덤 회전
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1), # 색상 조절
    transforms.ToTensor(),             # PIL Image를 PyTorch Tensor로 변환
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # 정규화
])

# 증강 적용 및 시각화
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.title("Original Image")
plt.imshow(img_pil)
plt.axis('off')

plt.subplot(1, 2, 2)
plt.title("Augmented Image")
# 증강된 이미지는 Tensor이므로 다시 PIL Image로 변환하여 시각화
# 정규화된 이미지를 다시 0-1 범위로 되돌려야 함
augmented_tensor = augment_transform(img_pil)
# 역정규화 (시각화를 위해)
inv_normalize = transforms.Normalize(
    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],
    std=[1/0.229, 1/0.224, 1/0.225]
)
augmented_img_for_display = inv_normalize(augmented_tensor).permute(1, 2, 0).numpy()
augmented_img_for_display = np.clip(augmented_img_for_display, 0, 1)
plt.imshow(augmented_img_for_display)
plt.axis('off')

plt.tight_layout()
plt.show()
```

#### 코드 예시 (Keras `ImageDataGenerator`)

```python
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt

# 예시 이미지 생성 (실제 사용 시에는 이미지 파일을 불러와야 함)
img_array = np.random.randint(0, 256, (128, 128, 3), dtype=np.uint8)
img_array = np.expand_dims(img_array, axis=0) # 배치 차원 추가

# 이미지 증강 제너레이터 정의
datagen = ImageDataGenerator(
    rotation_range=20,        # 20도 내에서 랜덤 회전
    width_shift_range=0.2,    # 20% 내에서 가로 이동
    height_shift_range=0.2,   # 20% 내에서 세로 이동
    shear_range=0.2,          # 20% 내에서 전단 변환
    zoom_range=0.2,           # 20% 내에서 확대/축소
    horizontal_flip=True,     # 좌우 반전
    fill_mode='nearest'       # 변환 시 빈 공간 채우는 방식
)

# 증강된 이미지 생성 및 시각화
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.title("Original Image")
plt.imshow(img_array[0])
plt.axis('off')

plt.subplot(1, 2, 2)
plt.title("Augmented Image")
# 첫 번째 증강된 이미지 가져오기
augmented_img = datagen.flow(img_array, batch_size=1)[0][0]
plt.imshow(augmented_img.astype(np.uint8))
plt.axis('off')

plt.tight_layout()
plt.show()
```

### 하이퍼파라미터 설명 (증강)

- `rotation_range`: 이미지를 랜덤하게 회전시킬 각도 범위 (0-180).
- `width_shift_range`, `height_shift_range`: 이미지를 가로/세로로 이동시킬 비율 (0-1) 또는 픽셀 수.
- `shear_range`: 전단 변환(Shear Transformation) 강도.
- `zoom_range`: 랜덤 확대/축소 범위.
- `horizontal_flip`, `vertical_flip`: 수평/수직 반전 적용 여부.
- `brightness_range`: 밝기 조절 범위.
- `fill_mode`: 변환 시 새로 생성되는 픽셀을 채우는 방식 (예: 'nearest', 'reflect', 'wrap', 'constant').

## CNN을 활용한 이미지 분류

### 개념

컨볼루션 신경망(Convolutional Neural Network, CNN)은 이미지와 같은 그리드(grid) 형태의 데이터를 처리하는 데 특화된 딥러닝 모델입니다. 주로 컨볼루션(Convolution) 계층, 풀링(Pooling) 계층, 활성화 함수(Activation Function), 완전 연결(Fully Connected) 계층으로 구성됩니다. CNN은 이미지의 공간적 계층적 특징을 자동으로 학습하여 이미지 분류, 객체 탐지, 이미지 분할 등 다양한 컴퓨터 비전 태스크에서 뛰어난 성능을 발휘합니다.

### 용도

- **이미지 분류**: 주어진 이미지가 어떤 클래스(범주)에 속하는지 예측합니다 (예: 고양이/개 분류, 질병 유무 판단).
- **객체 탐지**: 이미지 내에서 객체의 위치를 바운딩 박스로 표시하고 해당 객체의 클래스를 예측합니다.
- **이미지 분할**: 이미지의 각 픽셀이 어떤 객체에 속하는지 분류하여 이미지 영역을 분할합니다.

### 주의사항

- **데이터 양**: CNN은 많은 양의 학습 데이터가 필요합니다. 데이터가 부족할 경우 과적합이 발생하기 쉽습니다.
- **컴퓨팅 자원**: 복잡한 CNN 모델은 학습에 많은 GPU 메모리와 계산 시간을 요구합니다.
- **과적합**: 모델의 복잡도가 높거나 학습 데이터가 부족할 경우, 훈련 데이터에만 과도하게 맞춰져 새로운 데이터에 대한 성능이 저하될 수 있습니다. 드롭아웃(Dropout), 배치 정규화(Batch Normalization), 데이터 증강 등의 기법으로 완화할 수 있습니다.

### 코드 예시 (Keras)

```python
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.utils import to_categorical
import matplotlib.pyplot as plt

# 1. 데이터 로드 및 전처리
(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()

# 픽셀 값을 0-1 범위로 정규화
train_images = train_images.astype('float32') / 255.0
test_images = test_images.astype('float32') / 255.0

# 원-핫 인코딩
train_labels = to_categorical(train_labels, 10)
test_labels = to_categorical(test_labels, 10)

# 2. CNN 모델 구축
model_keras = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(10, activation='softmax') # 10개 클래스 분류
])

# 3. 모델 컴파일
model_keras.compile(optimizer='adam',
                    loss='categorical_crossentropy',
                    metrics=['accuracy'])

# 4. 모델 학습
history_keras = model_keras.fit(train_images, train_labels, epochs=10, 
                                validation_data=(test_images, test_labels))

# 5. 모델 평가
test_loss, test_acc = model_keras.evaluate(test_images, test_labels, verbose=2)
print(f"\nKeras 테스트 정확도: {test_acc:.4f}")

# 6. 학습 과정 시각화
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history_keras.history['accuracy'], label='accuracy')
plt.plot(history_keras.history['val_accuracy'], label='val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Keras Model Accuracy')

plt.subplot(1, 2, 2)
plt.plot(history_keras.history['loss'], label='loss')
plt.plot(history_keras.history['val_loss'], label='val_loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('Keras Model Loss')
plt.tight_layout()
plt.show()
```

### 코드 예시 (PyTorch)

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt

# 1. 데이터 로드 및 전처리
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # -1 ~ 1 범위로 정규화
])

train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

# 2. CNN 모델 구축
class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, padding=1)
        self.fc1 = nn.Linear(64 * 4 * 4, 64) # CIFAR-10 이미지 크기 32x32 -> 풀링 2번 후 4x4
        self.fc2 = nn.Linear(64, 10)

    def forward(self, x):
        x = self.pool(torch.relu(self.conv1(x)))
        x = self.pool(torch.relu(self.conv2(x)))
        x = torch.relu(self.conv3(x))
        x = x.view(-1, 64 * 4 * 4) # Flatten
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

model_pytorch = SimpleCNN()

# 3. 손실 함수 및 옵티마이저 설정
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model_pytorch.parameters(), lr=0.001)

# 4. 모델 학습
num_epochs = 10
train_losses = []
train_accuracies = []
val_losses = []
val_accuracies = []

for epoch in range(num_epochs):
    model_pytorch.train()
    running_loss = 0.0
    correct_train = 0
    total_train = 0
    for i, (inputs, labels) in enumerate(train_loader):
        optimizer.zero_grad()
        outputs = model_pytorch(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)
        total_train += labels.size(0)
        correct_train += (predicted == labels).sum().item()
    
    train_losses.append(running_loss / len(train_loader))
    train_accuracies.append(correct_train / total_train)

    model_pytorch.eval()
    correct_val = 0
    total_val = 0
    val_loss = 0.0
    with torch.no_grad():
        for inputs, labels in test_loader:
            outputs = model_pytorch(inputs)
            loss = criterion(outputs, labels)
            val_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total_val += labels.size(0)
            correct_val += (predicted == labels).sum().item()
    
    val_losses.append(val_loss / len(test_loader))
    val_accuracies.append(correct_val / total_val)

    print(f'Epoch [{epoch+1}/{num_epochs}], '\
          f'Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accuracies[-1]:.4f}, '\
          f'Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accuracies[-1]:.4f}')

# 5. 모델 평가
print("\n--- PyTorch 모델 평가 ---")
model_pytorch.eval()
correct = 0
total = 0
with torch.no_grad():
    for images, labels in test_loader:
        outputs = model_pytorch(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print(f'PyTorch 테스트 정확도: {100 * correct / total:.2f}%')

# 6. 학습 과정 시각화
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(train_accuracies, label='Train Accuracy')
plt.plot(val_accuracies, label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.title('PyTorch Model Accuracy')

plt.subplot(1, 2, 2)
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('PyTorch Model Loss')
plt.tight_layout()
plt.show()
```

### `pytorch` 및 `keras`에서 제공하는 이미지 모델 종류 및 활용

#### 전이 학습 (Transfer Learning)

전이 학습은 대규모 데이터셋(예: ImageNet)으로 미리 학습된 모델의 가중치를 가져와 새로운(작은) 데이터셋에 맞게 미세 조정(Fine-tuning)하는 기법입니다. 이는 적은 데이터로도 높은 성능을 달성할 수 있게 해주며, 학습 시간을 단축시킵니다.

#### Keras/TensorFlow `tf.keras.applications`

`tf.keras.applications` 모듈은 ImageNet 데이터셋으로 사전 학습된 다양한 CNN 모델(예: VGG16, ResNet50, InceptionV3, MobileNetV2 등)을 제공합니다. 이 모델들은 특징 추출기(Feature Extractor)로 사용되거나, 새로운 분류 계층을 추가하여 전이 학습에 활용될 수 있습니다.

```python
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# 1. 데이터 준비 (예시: 작은 이미지 데이터셋)
# 실제 사용 시에는 ImageDataGenerator.flow_from_directory 등을 사용
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()
x_train = tf.image.resize(x_train, (224, 224)) # ResNet50 입력 크기에 맞춤
x_test = tf.image.resize(x_test, (224, 224))
x_train = x_train / 255.0
x_test = x_test / 255.0
y_train = tf.keras.utils.to_categorical(y_train, 10)
y_test = tf.keras.utils.to_categorical(y_test, 10)

# 2. 사전 학습된 ResNet50 모델 로드 (가중치 동결)
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
base_model.trainable = False # 사전 학습된 가중치 동결

# 3. 새로운 분류 계층 추가
inputs = tf.keras.Input(shape=(224, 224, 3))
x = base_model(inputs, training=False)
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dense(10, activation='softmax')(x) # 새로운 데이터셋의 클래스 수에 맞게 조정
outputs = x

model_transfer_keras = models.Model(inputs, outputs)

# 4. 모델 컴파일 및 학습
model_transfer_keras.compile(optimizer='adam',
                             loss='categorical_crossentropy',
                             metrics=['accuracy'])

history_transfer_keras = model_transfer_keras.fit(x_train, y_train, epochs=5, 
                                                  validation_data=(x_test, y_test))

# 5. 모델 평가
test_loss, test_acc = model_transfer_keras.evaluate(x_test, y_test, verbose=2)
print(f"\nKeras 전이 학습 모델 테스트 정확도: {test_acc:.4f}")
```

#### PyTorch `torchvision.models`

`torchvision.models` 모듈은 ImageNet으로 사전 학습된 다양한 모델(예: `resnet18`, `vgg16`, `mobilenet_v2` 등)을 제공합니다. 이 모델들도 Keras와 유사하게 특징 추출기로 사용되거나, 마지막 분류 계층을 교체하여 전이 학습에 활용됩니다.

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import models, datasets, transforms
from torch.utils.data import DataLoader

# 1. 데이터 준비 (예시: 작은 이미지 데이터셋)
transform_transfer = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224), # ResNet 입력 크기에 맞춤
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

train_dataset_transfer = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_transfer)
test_dataset_transfer = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_transfer)

train_loader_transfer = DataLoader(train_dataset_transfer, batch_size=32, shuffle=True)
test_loader_transfer = DataLoader(test_dataset_transfer, batch_size=32, shuffle=False)

# 2. 사전 학습된 ResNet18 모델 로드 (가중치 동결)
model_transfer_pytorch = models.resnet18(pretrained=True) # pretrained=True로 사전 학습된 가중치 로드

# 마지막 완전 연결 계층(fc)을 새로운 클래스 수에 맞게 수정
num_ftrs = model_transfer_pytorch.fc.in_features
model_transfer_pytorch.fc = nn.Linear(num_ftrs, 10) # CIFAR-10은 10개 클래스

# 특징 추출기 부분의 가중치 동결 (선택 사항, 일반적으로는 미세 조정을 위해 동결하지 않음)
# for param in model_transfer_pytorch.parameters():
#     param.requires_grad = False
# model_transfer_pytorch.fc.weight.requires_grad = True
# model_transfer_pytorch.fc.bias.requires_grad = True

# 3. 손실 함수 및 옵티마이저 설정
criterion_transfer = nn.CrossEntropyLoss()
optimizer_transfer = optim.Adam(model_transfer_pytorch.parameters(), lr=0.001)

# 4. 모델 학습
num_epochs_transfer = 5
for epoch in range(num_epochs_transfer):
    model_transfer_pytorch.train()
    running_loss = 0.0
    for i, (inputs, labels) in enumerate(train_loader_transfer):
        optimizer_transfer.zero_grad()
        outputs = model_transfer_pytorch(inputs)
        loss = criterion_transfer(outputs, labels)
        loss.backward()
        optimizer_transfer.step()
        running_loss += loss.item()
    
    print(f'Epoch [{epoch+1}/{num_epochs_transfer}], Loss: {running_loss / len(train_loader_transfer):.4f}')

# 5. 모델 평가
print("\n--- PyTorch 전이 학습 모델 평가 ---")
model_transfer_pytorch.eval()
correct_transfer = 0
total_transfer = 0
with torch.no_grad():
    for images, labels in test_loader_transfer:
        outputs = model_transfer_pytorch(images)
        _, predicted = torch.max(outputs.data, 1)
        total_transfer += labels.size(0)
        correct_transfer += (predicted == labels).sum().item()

print(f'PyTorch 전이 학습 모델 테스트 정확도: {100 * correct_transfer / total_transfer:.2f}%')
```

## 장단점 및 대안

| 기법 | 장점 | 단점 |
|---|---|---|
| **CNN** | - 이미지의 공간적, 계층적 특징 자동 학습<br>- 이미지 분류, 객체 탐지 등 다양한 태스크에 뛰어난 성능<br>- 전이 학습을 통해 적은 데이터로도 높은 성능 달성 가능 | - 대규모 데이터셋과 컴퓨팅 자원 필요<br>- 모델 구조가 복잡하여 학습 시간이 김<br>- 과적합 발생 가능성 |
| **데이터 증강** | - 데이터셋 크기 인위적 확장으로 과적합 방지 및 일반화 성능 향상<br>- 모델의 강인성(robustness) 증가 | - 부적절한 증강은 정보 손실 및 왜곡 초래<br>- 도메인 지식 기반의 신중한 선택 필요 |

### 대안

- **Vision Transformer (ViT)**: CNN의 대안으로, 이미지 패치를 시퀀스로 변환하여 트랜스포머 모델을 적용하는 방식입니다. 대규모 데이터셋에서 CNN보다 더 좋은 성능을 보이는 경우가 많습니다.
- **전통적인 머신러닝 기법**: SIFT, HOG와 같은 수동으로 설계된 특징(hand-crafted features)을 추출한 후 SVM, Random Forest 등의 전통적인 머신러닝 모델을 적용하는 방법입니다. 딥러닝 이전에 주로 사용되었으나, 현재는 딥러닝 모델에 비해 성능이 떨어지는 경우가 많습니다.
