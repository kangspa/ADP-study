### 연관 규칙 분석 (Association Rule Mining)

#### 개념 요약
연관 규칙 분석은 대규모 데이터 항목 집합(e.g., 마트 거래 기록, 웹사이트 방문 기록) 속에서 항목들 간의 유용한 관계, 즉 '연관 규칙'을 발견하는 비지도 학습 방법입니다. 가장 대표적인 예시는 "장바구니 분석"으로, "만약 고객이 '기저귀'를 구매했다면, '맥주'도 함께 구매할 가능성이 높다"와 같은 규칙을 찾아내는 것입니다.

연관 규칙은 **'만약 {A}이면, {B}이다' ($A \rightarrow B$)** 형태를 가집니다.
- **A**: 선행 항목 (Antecedent)
- **B**: 후행 항목 (Consequent)

이러한 규칙의 유용성을 평가하기 위해 다음 세 가지 핵심 지표가 사용됩니다.

1.  **지지도 (Support)**:
    - **개념**: 전체 거래 중, 항목 A와 항목 B를 **모두 포함**하는 거래의 비율.
    - **수식**: $Support(A \rightarrow B) = \frac{P(A \cap B)}{전체 거래 수}$
    - **의미**: 규칙이 데이터에서 얼마나 자주 발생하는지를 나타냅니다. 지지도가 너무 낮으면 우연히 발생한 규칙일 가능성이 높습니다.

2.  **신뢰도 (Confidence)**:
    - **개념**: 항목 A를 포함하는 거래 중에서, 항목 B도 **함께 포함**하는 거래의 비율 (조건부 확률).
    - **수식**: $Confidence(A \rightarrow B) = \frac{P(A \cap B)}{P(A)} = \frac{Support(A, B)}{Support(A)}$
    - **의미**: A를 구매했을 때 B를 구매할 조건부 확률을 나타냅니다. 규칙의 신뢰성을 측정합니다.

3.  **향상도 (Lift)**:
    - **개념**: 항목 B를 단독으로 구매할 확률 대비, 항목 A를 구매했을 때 항목 B를 구매할 확률의 증가 비율.
    - **수식**: $Lift(A \rightarrow B) = \frac{Confidence(A \rightarrow B)}{Support(B)} = \frac{P(B|A)}{P(B)}$
    - **의미**:
        - `Lift > 1`: A와 B는 양의 상관관계를 가집니다. (A를 구매하면 B를 구매할 확률이 높아짐)
        - `Lift = 1`: A와 B는 서로 독립적입니다. (A 구매가 B 구매에 영향을 주지 않음)
        - `Lift < 1`: A와 B는 음의 상관관계를 가집니다. (A를 구매하면 B를 구매할 확률이 낮아짐)

#### Apriori 알고리즘
Apriori는 연관 규칙 분석을 위한 가장 대표적인 알고리즘입니다. 모든 가능한 항목 집합을 탐색하는 것은 계산적으로 불가능하므로, Apriori는 **'빈번하지 않은 항목 집합의 부분 집합 또한 빈번하지 않다'**는 원리를 사용하여 탐색 범위를 효율적으로 줄입니다.

- **동작 방식**:
  1. 사용자가 지정한 **최소 지지도(minimum support)**를 설정합니다.
  2. 최소 지지도를 넘는 개별 항목들(1-itemset)만 찾습니다.
  3. 1단계에서 찾은 항목들을 조합하여 2-itemset을 만들고, 이 중에서 최소 지지도를 넘는 것들만 찾습니다.
  4. 이 과정을 반복하여 최소 지지도를 만족하는 모든 **빈번한 항목 집합(frequent itemsets)**을 찾습니다.
  5. 찾아낸 빈번한 항목 집합들을 기반으로, 사용자가 지정한 **최소 신뢰도(minimum confidence)**를 넘는 연관 규칙들을 생성합니다.

#### 적용 가능한 상황
- 마트, 온라인 쇼핑몰 등에서 고객의 구매 패턴을 분석하여 교차 판매(cross-selling) 전략을 수립할 때.
- 웹사이트 로그 분석을 통해 사용자의 페이지 이동 경로를 파악하고 웹사이트 구조를 개선할 때.
- 의료 데이터에서 특정 질병과 증상 간의 연관성을 분석할 때.

#### 구현 방법
`scikit-learn`에는 Apriori 알고리즘이 직접 포함되어 있지 않습니다. 따라서 외부 라이브러리인 `mlxtend`를 사용하는 것이 일반적입니다.

##### 주의사항
- **데이터 형태**: Apriori 알고리즘을 사용하려면 입력 데이터를 **트랜잭션(Transaction) 형태**로 변환해야 합니다. 각 행은 하나의 거래(transaction)를, 각 열은 하나의 항목(item)을 나타내며, 해당 거래에 항목이 포함되었는지 여부를 True/False 또는 1/0으로 표시하는 형태여야 합니다.
- **최소 지지도/신뢰도 설정**: `min_support`와 `min_confidence` 값을 어떻게 설정하느냐에 따라 결과의 양과 질이 크게 달라집니다. 너무 낮게 설정하면 의미 없는 규칙이 너무 많이 생성되고, 너무 높게 설정하면 유용한 규칙을 놓칠 수 있습니다.

##### 코드 예시 (`mlxtend`)
```python
# !pip install mlxtend
import pandas as pd
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules

# 1. 데이터 준비 (리스트의 리스트 형태)
dataset = [['Milk', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],
           ['Dill', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],
           ['Milk', 'Apple', 'Kidney Beans', 'Eggs'],
           ['Milk', 'Unicorn', 'Corn', 'Kidney Beans', 'Yogurt'],
           ['Corn', 'Onion', 'Onion', 'Kidney Beans', 'Ice cream', 'Eggs']]

# 2. 데이터를 트랜잭션 형태로 변환
te = TransactionEncoder()
te_ary = te.fit(dataset).transform(dataset)
df = pd.DataFrame(te_ary, columns=te.columns_)
print("--- Transaction Data ---")
print(df)

# 3. Apriori 알고리즘을 이용해 빈번한 항목 집합 찾기
# min_support: 최소 지지도.
# use_colnames=True: 항목 이름을 컬럼명으로 사용.
frequent_itemsets = apriori(df, min_support=0.6, use_colnames=True)
print("\n--- Frequent Itemsets (min_support >= 0.6) ---")
print(frequent_itemsets)

# 4. 연관 규칙 생성
# metric: 규칙을 필터링할 기준 ('confidence', 'lift' 등).
# min_threshold: metric의 최소 임계값.
rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.7)
print("\n--- Association Rules (min_confidence >= 0.7) ---")
# antecedents: 선행 항목, consequents: 후행 항목
# antecedent support, consequent support: 각 항목 집합의 지지도
# support, confidence, lift: 규칙의 지지도, 신뢰도, 향상도
# leverage, conviction: 다른 연관성 척도
print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])
```

#### 장단점 및 대안
- **장점**:
    - 결과가 'If-Then' 형태의 규칙으로 나와 해석이 매우 직관적이고 비즈니스에 적용하기 쉽습니다.
    - 비지도 학습으로, 데이터의 숨겨진 패턴을 발견하는 데 유용합니다.
- **단점**:
    - 데이터셋이 크거나 항목의 종류가 많아지면 가능한 모든 조합을 탐색해야 하므로 계산량이 기하급수적으로 증가합니다.
    - `min_support` 설정에 따라 성능과 결과가 민감하게 변합니다.
- **대안**:
    - **FP-Growth (Frequent Pattern-Growth)**: Apriori의 단점인 후보 항목 집합 생성 과정을 개선한 알고리즘입니다. FP-Tree라는 자료 구조를 사용하여 Apriori보다 더 빠른 속도로 빈번한 항목 집합을 찾습니다.
