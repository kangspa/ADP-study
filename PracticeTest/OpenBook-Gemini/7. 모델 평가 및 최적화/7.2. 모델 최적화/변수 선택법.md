# 변수 선택법: 후진 제거법, 전진 선택법

## 개념 요약

변수 선택법(Feature Selection)은 머신러닝 모델의 성능을 향상시키고, 과적합을 방지하며, 모델의 해석력을 높이기 위해 데이터셋에서 가장 관련성이 높거나 유용한 피처(변수)의 부분집합을 선택하는 과정입니다. 불필요하거나 중복되는 피처를 제거함으로써 모델의 복잡도를 줄이고, 학습 시간을 단축하며, 일반화 성능을 개선하는 것을 목표로 합니다.

## 적용 가능한 상황

- **고차원 데이터**: 피처의 수가 샘플 수보다 많거나 매우 많은 경우, 차원의 저주(Curse of Dimensionality) 문제를 해결하기 위해 사용됩니다.
- **노이즈가 많은 데이터**: 모델 성능에 부정적인 영향을 미치는 불필요한 피처를 제거하여 모델의 강건성(robustness)을 높일 때.
- **모델 해석력 향상**: 모델의 예측에 기여하는 핵심 피처만을 남겨 모델의 의사결정 과정을 더 쉽게 이해하고 설명할 수 있도록 할 때.
- **과적합 방지**: 불필요한 피처가 모델에 과도하게 학습되는 것을 방지하여 새로운 데이터에 대한 예측 성능을 높일 때.
- **학습 시간 단축 및 계산 비용 절감**: 피처의 수를 줄여 모델 학습 및 예측에 필요한 시간과 자원을 절약할 때.

## 구현 방법

변수 선택법은 크게 필터(Filter) 방식, 래퍼(Wrapper) 방식, 임베디드(Embedded) 방식으로 나눌 수 있습니다. 후진 제거법과 전진 선택법은 래퍼 방식에 해당합니다.

### 1. 후진 제거법 (Backward Elimination)

#### 용도

후진 제거법은 모든 피처를 포함한 모델에서 시작하여, 통계적으로 가장 유의미하지 않은(예: p-value가 가장 높은) 피처를 하나씩 제거해나가면서 모델의 성능이 가장 좋아지는 피처 조합을 찾는 방법입니다. 각 단계에서 피처를 제거할 때마다 모델을 재학습하고 성능을 평가합니다.

#### 주의사항

- **계산 비용**: 피처의 수가 많을수록 각 단계에서 모델을 재학습해야 하므로 계산 비용이 많이 들 수 있습니다.
- **지역 최적해**: 한 번 제거된 피처는 다시 추가되지 않으므로, 전역 최적해(Global Optimum)가 아닌 지역 최적해(Local Optimum)에 빠질 위험이 있습니다.
- **상호작용 효과 무시**: 제거된 피처가 다른 피처와 중요한 상호작용 효과를 가질 수 있음에도 불구하고 이를 고려하지 못할 수 있습니다.

#### 코드 예시 (`statsmodels`를 이용한 OLS 모델)

```python
import numpy as np
import pandas as pd
import statsmodels.api as sm
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# 1. 데이터 준비
X, y = make_regression(n_samples=100, n_features=10, n_informative=5, random_state=42)
feature_names = [f'X{i}' for i in range(X.shape[1])]
X_df = pd.DataFrame(X, columns=feature_names)
X_df = sm.add_constant(X_df) # 상수항 추가

X_train, X_test, y_train, y_test = train_test_split(X_df, y, test_size=0.2, random_state=42)

def backward_elimination(X_data, y_data, significance_level=0.05):
    features = list(X_data.columns)
    while len(features) > 1: # 상수항(const)은 남겨둠
        model = sm.OLS(y_data, X_data[features]).fit()
        p_values = model.pvalues
        max_p_value = p_values.drop('const', errors='ignore').max() # 상수항 제외
        
        if max_p_value > significance_level:
            redundant_feature = p_values.drop('const', errors='ignore').idxmax()
            features.remove(redundant_feature)
            print(f"제거된 피처: {redundant_feature}, p-value: {max_p_value:.4f}")
        else:
            break
    return features

# 2. 후진 제거법 적용
print("--- 후진 제거법 시작 ---")
selected_features_be = backward_elimination(X_train, y_train)
print(f"최종 선택된 피처: {selected_features_be}")

# 3. 최종 모델 학습 및 평가
final_model_be = sm.OLS(y_train, X_train[selected_features_be]).fit()
print("\n--- 후진 제거법 최종 모델 요약 ---")
print(final_model_be.summary())

y_pred_be = final_model_be.predict(X_test[selected_features_be])
mse_be = mean_squared_error(y_test, y_pred_be)
print(f"테스트 세트 MSE (후진 제거법): {mse_be:.4f}")
```

#### 하이퍼파라미터 설명

- `significance_level`: 피처를 제거할지 결정하는 유의수준입니다. 일반적으로 0.05를 사용하며, 이 값보다 p-value가 높으면 해당 피처는 통계적으로 유의미하지 않다고 판단하여 제거합니다.

#### 결과 해석 방법

- `final_model_be.summary()`: 최종 선택된 피처들로 학습된 모델의 통계적 요약을 제공합니다. 각 피처의 계수, p-value, R-squared 값 등을 확인할 수 있습니다.
- `mean_squared_error`: 최종 모델의 예측 성능을 평가합니다. 후진 제거법을 통해 선택된 피처들이 모델의 예측 오차를 얼마나 줄였는지 확인할 수 있습니다.

---

### 2. 전진 선택법 (Forward Selection)

#### 용도

전진 선택법은 아무 피처도 없는 모델에서 시작하여, 통계적으로 가장 유의미한(예: p-value가 가장 낮은) 피처를 하나씩 추가해나가면서 모델의 성능이 가장 좋아지는 피처 조합을 찾는 방법입니다. 각 단계에서 피처를 추가할 때마다 모델을 재학습하고 성능을 평가합니다.

#### 주의사항

- **계산 비용**: 후진 제거법과 마찬가지로 피처의 수가 많을수록 계산 비용이 많이 들 수 있습니다.
- **지역 최적해**: 한 번 추가된 피처는 다시 제거되지 않으므로, 전역 최적해(Global Optimum)가 아닌 지역 최적해에 빠질 위험이 있습니다.
- **중복 피처**: 이미 추가된 피처와 강한 상관관계를 가지는 피처가 추가될 경우, 다중공선성 문제를 야기할 수 있습니다.

#### 코드 예시 (`statsmodels`를 이용한 OLS 모델)

```python
import numpy as np
import pandas as pd
import statsmodels.api as sm
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# 1. 데이터 준비 (후진 제거법 예시와 동일)
X, y = make_regression(n_samples=100, n_features=10, n_informative=5, random_state=42)
feature_names = [f'X{i}' for i in range(X.shape[1])]
X_df = pd.DataFrame(X, columns=feature_names)
X_df = sm.add_constant(X_df) # 상수항 추가

X_train, X_test, y_train, y_test = train_test_split(X_df, y, test_size=0.2, random_state=42)

def forward_selection(X_data, y_data, significance_level=0.05):
    initial_features = ['const'] # 상수항부터 시작
    remaining_features = list(set(X_data.columns) - set(initial_features))
    selected_features = list(initial_features)

    while remaining_features:
        best_p_value = 1.0
        best_feature = None
        
        for feature in remaining_features:
            model = sm.OLS(y_data, X_data[selected_features + [feature]]).fit()
            p_value = model.pvalues[feature]
            
            if p_value < best_p_value:
                best_p_value = p_value
                best_feature = feature
        
        if best_feature and best_p_value < significance_level:
            selected_features.append(best_feature)
            remaining_features.remove(best_feature)
            print(f"추가된 피처: {best_feature}, p-value: {best_p_value:.4f}")
        else:
            break
    return selected_features

# 2. 전진 선택법 적용
print("--- 전진 선택법 시작 ---")
selected_features_fs = forward_selection(X_train, y_train)
print(f"최종 선택된 피처: {selected_features_fs}")

# 3. 최종 모델 학습 및 평가
final_model_fs = sm.OLS(y_train, X_train[selected_features_fs]).fit()
print("\n--- 전진 선택법 최종 모델 요약 ---")
print(final_model_fs.summary())

y_pred_fs = final_model_fs.predict(X_test[selected_features_fs])
mse_fs = mean_squared_error(y_test, y_pred_fs)
print(f"테스트 세트 MSE (전진 선택법): {mse_fs:.4f}")
```

#### 하이퍼파라미터 설명

- `significance_level`: 피처를 추가할지 결정하는 유의수준입니다. 일반적으로 0.05를 사용하며, 이 값보다 p-value가 낮으면 해당 피처는 통계적으로 유의미하다고 판단하여 추가합니다.

#### 결과 해석 방법

- `final_model_fs.summary()`: 최종 선택된 피처들로 학습된 모델의 통계적 요약을 제공합니다. 각 피처의 계수, p-value, R-squared 값 등을 확인할 수 있습니다.
- `mean_squared_error`: 최종 모델의 예측 성능을 평가합니다. 전진 선택법을 통해 선택된 피처들이 모델의 예측 오차를 얼마나 줄였는지 확인할 수 있습니다.

## 장단점 및 대안

| 방법 | 장점 | 단점 |
|---|---|---|
| **후진 제거법** | - 모든 피처를 고려하여 시작하므로, 중요한 피처가 누락될 가능성이 적음 | - 계산 비용이 높음<br>- 지역 최적해에 빠질 수 있음<br>- 피처 간 상호작용을 고려하기 어려움 |
| **전진 선택법** | - 계산 비용이 상대적으로 낮음<br>- 구현이 직관적 | - 지역 최적해에 빠질 수 있음<br>- 한 번 추가된 피처는 제거되지 않음<br>- 중요한 피처가 초기에 선택되지 않으면 누락될 수 있음 |

### 대안

- **단계적 선택법 (Stepwise Selection)**: 전진 선택법과 후진 제거법을 결합한 방식으로, 각 단계에서 피처를 추가하거나 제거하면서 최적의 피처 조합을 찾습니다. 가장 널리 사용되는 방법 중 하나입니다.
- **재귀적 피처 제거 (Recursive Feature Elimination, RFE)**: 모델을 반복적으로 학습시키면서 중요도가 가장 낮은 피처를 하나씩 제거하여 최적의 피처 조합을 찾는 방법입니다. `sklearn.feature_selection.RFE`를 사용합니다.
- **L1 정규화 (Lasso Regression)**: 모델 학습 과정에서 중요도가 낮은 피처의 계수를 0으로 만들어 자동으로 피처 선택을 수행합니다. `sklearn.linear_model.Lasso`를 사용합니다.
- **트리 기반 피처 선택**: 트리 기반 모델의 `feature_importances_`를 활용하여 중요도가 높은 피처를 선택합니다. `sklearn.feature_selection.SelectFromModel`을 사용할 수 있습니다.
- **필터 방식 (Filter Methods)**: 피처와 타겟 변수 간의 통계적 관계(예: 상관계수, 카이제곱 통계량)를 기반으로 피처를 선택합니다. 모델 학습 없이 독립적으로 피처를 평가하므로 계산이 빠릅니다. `sklearn.feature_selection.SelectKBest`, `SelectPercentile` 등을 사용합니다.
